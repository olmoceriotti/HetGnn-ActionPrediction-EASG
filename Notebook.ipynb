{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr1CcbrkvMx0"
      },
      "source": [
        "# Action prediction on EASG using GNNs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T78NFVBva4s"
      },
      "source": [
        "In questo notebook sono poste le basi per eseguire il task di action prediction su EASG. Per rendere il dataset originale compatibile con PyTorch Geometric, è stato convertito in un insieme di tensori rappresentanti nodi e connessioni. Per eseguire l'embedding di verbi, oggetti e relazioni si è utilizzato un dizionario, convertendo la parola in questione nell'indice della riga dove è situata. I grafi sono rappresentati nel modo piu fedele possibile rispetto al dataset originale. Per rispettare la metodologia proposta nel paper, i grafi rappresentanti le scene sono suddivisi in sottografi rappresentanti un numero regolabile di grafi consecutivi. Sono state aggiunte, per massimizzare le informazioni alcune features. In primis, al nodo CW, sono state aggiunte le connessioni con oggetti non partecipanti nell'azione, in quanto possono essere parte di azioni successive.\n",
        "Inoltre, nella creazione dei sottografi, è stata inclusa una relazione con gli oggetti visti nei grafi precedenti al sottografo in questione, in modo da tenere conto dell'esperienza pregressa dell'osservatore. Il task in questione rispetta quanto proposto nel paper, ma il modello che si utilizzerà sarà radicalmente diverso rispetto a GPT. I file originali sono stati modificati rimuovendo grafi non conformi e aggiungendo annotazioni necessarie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl8KUAHXPv6C"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycoZ4dam6C1R",
        "outputId": "88e809ef-0014-4b39-9150-9e4091737c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.9.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.9.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.4\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.3.post0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install wandb\n",
        "!pip install torchmetrics\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0NTR6FCtB8J"
      },
      "outputs": [],
      "source": [
        "base_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3bBhsjE7VQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6587c4c-cacf-41d0-b533-1f1d943481c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/EASG/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CiSZxCYrQ6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf8ca18-4b2f-4916-e3dd-30abb845c3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukL61VIJQEqN"
      },
      "source": [
        "## Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqAx6w_4SYah"
      },
      "outputs": [],
      "source": [
        "with open(base_path + 'easg.json', 'r') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSYCKd9pKfkr",
        "outputId": "472d82ea-e855-486a-c97b-76419dcd6559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "483\n",
            "227\n",
            "16\n"
          ]
        }
      ],
      "source": [
        "def create_word_index_dict(file_path):\n",
        "    word_index_dict = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        for index, line in enumerate(file, start=1):\n",
        "            word = line.strip()\n",
        "            word_index_dict[word] = index\n",
        "    return word_index_dict\n",
        "\n",
        "objects_path = base_path + 'objects.txt'\n",
        "verbs_path = base_path + 'verbs.txt'\n",
        "relationships_path = base_path + 'relationships.txt'\n",
        "\n",
        "o_dict = create_word_index_dict(objects_path)\n",
        "v_dict = create_word_index_dict(verbs_path)\n",
        "r_dict = create_word_index_dict(relationships_path)\n",
        "\n",
        "print(len(o_dict))\n",
        "print(len(v_dict))\n",
        "print(len(r_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16FN_rwviKbG"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "verb_feats = torch.load(base_path + 'verb_features.pt')\n",
        "with open(base_path + 'roi_feats_val.pkl', 'rb') as f:\n",
        "  roi_val_feats = pickle.load(f)\n",
        "with open(base_path + 'roi_feats_train.pkl', 'rb') as f:\n",
        "  roi_train_feats = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4FE-ptpP9nN"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJlF-NQFdXk"
      },
      "outputs": [],
      "source": [
        "def rindex(lst, item):\n",
        "    try:\n",
        "        return len(lst) - 1 - lst[::-1].index(item)\n",
        "    except ValueError:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01DMLrQR-dOd"
      },
      "outputs": [],
      "source": [
        "def calc_possible_seq(seq_len, data_obj_list):\n",
        "  non_valid = 0\n",
        "  valid = 0\n",
        "  tot_seq = 0\n",
        "  for data in data_obj_list:\n",
        "    if len(data.extra_features[\"graph_uids\"]) <= seq_len:\n",
        "      non_valid = non_valid + 1\n",
        "    if len(data.extra_features[\"graph_uids\"]) > seq_len:\n",
        "      valid = valid + 1\n",
        "      tot_seq = tot_seq + (len(data.extra_features[\"graph_uids\"]) - seq_len)\n",
        "  return non_valid, valid, tot_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrAnvig7qdPq"
      },
      "outputs": [],
      "source": [
        "def create_mask(original, id_list):\n",
        "    if isinstance(original, list):\n",
        "        return torch.tensor([x in id_list for x in original], dtype=torch.bool)\n",
        "    elif isinstance(original, torch.Tensor):\n",
        "        return torch.tensor([x.item() in id_list for x in original], dtype=torch.bool)\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "def init_model(model):\n",
        "    for name, param in model.named_parameters():\n",
        "          if 'weight' in name:\n",
        "            if 'lin' in name or 'linear' in name:\n",
        "              init.kaiming_normal_(param, mode='fan_in', nonlinearity='relu')"
      ],
      "metadata": {
        "id": "S5jfkjkxLEZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLuwzrFMQgSQ"
      },
      "source": [
        "## Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6loZJIT-mflO"
      },
      "outputs": [],
      "source": [
        "def extract_from_sequence(seq, include_extracted_features = False):\n",
        "  num_obj_features = 13\n",
        "  num_extra_features = 1\n",
        "  num_verb_features = 2305\n",
        "  # One per sequence\n",
        "  objects = [] #semplice contatore\n",
        "  seen_objects = []\n",
        "  CW_to_seen = [[], []]\n",
        "  verbs = [] #funge da verb feature\n",
        "  v_to_o_edges = [[], []] # usa index in object feature pper identificare gli oggetti\n",
        "  v_to_o_attr = [] # usa index dell'edge per indetificare edge\n",
        "  o_to_o_edges = [[],[]]\n",
        "  o_to_o_attr = []\n",
        "  time_edges = [[],[]] # usa index oggetti, non ha features\n",
        "  dobj_edge = [[],[]]\n",
        "  previous_graph_objects = {} # struttura di controllo\n",
        "  current_graph_objects = {} # struttura di controllo\n",
        "  current_seen_objects = {}\n",
        "  objects_features = torch.empty(0, num_obj_features, dtype=torch.float) # unico index degli oggetti nella struttura finale\n",
        "  seen_objects_features = torch.empty(0, num_obj_features, dtype=torch.float)\n",
        "  graph_index_list = []\n",
        "  verb_uids = []\n",
        "  seen_obj_uid = []\n",
        "  CW_uids = []\n",
        "  extra_features = {\"split\": seq[\"split\"], \"video_uid\": seq[\"video_uid\"], \"shape\": [seq[\"W\"], seq[\"H\"]], \"graph_uids\": []}\n",
        "\n",
        "  for g in seq['graphs']:\n",
        "    extra_features['graph_uids'].append(g['graph_uid'])\n",
        "    current_verb = \"\" # reset\n",
        "    o_count = 0\n",
        "    duplicate_index = -1\n",
        "    CW_uids.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "    # Triplets\n",
        "    for ent1, rel, ent2 in g['triplets']:\n",
        "      if rel != 'verb':\n",
        "        if ent1 in v_dict.keys(): # Verb su Object\n",
        "          if current_verb == \"\": #prima tripla\n",
        "            current_verb = v_dict[ent1]\n",
        "            verbs.append(current_verb) #verbo è scelto per questo grafo\n",
        "            verb_uids.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "          elif current_verb != v_dict[ent1]:\n",
        "            print(f\"Error a: two verbs present {ent1} {ent2} {current_verb}\")\n",
        "            print(g['graph_uid'])\n",
        "            sys.exit()\n",
        "          if rel == 'dobj':\n",
        "            dobj_edge[0].append(len(verbs) - 1)\n",
        "            if o_dict[ent2] not in current_graph_objects.keys():\n",
        "              o_count = o_count + 1\n",
        "              current_graph_objects[o_dict[ent2]] = len(objects)\n",
        "              objects.append(o_dict[ent2])\n",
        "            dobj_edge[1].append(rindex(objects,o_dict[ent2]))\n",
        "          else:\n",
        "            v_to_o_edges[0].append(len(verbs) - 1) #ci si riferisce al verbo tramite l'indice (è ultimo)\n",
        "\n",
        "            if \":1\" in ent2: # Gestire caso in cui :1 viene prima del'oggetto originale\n",
        "              ent2 = ent2[:len(ent2)- 2]\n",
        "              o_count = o_count + 1\n",
        "              duplicate_index = len(objects)\n",
        "              objects.append(o_dict[ent2])\n",
        "              current_graph_objects[o_dict[ent2] + len(o_dict.keys())] = rindex(objects, o_dict[ent2]) #usato solo in costruzione grafo, nel tensore oggetto avrà indice corretto\n",
        "\n",
        "            elif o_dict[ent2] not in current_graph_objects.keys():\n",
        "              o_count = o_count + 1\n",
        "              current_graph_objects[o_dict[ent2]] = len(objects)\n",
        "              objects.append(o_dict[ent2])\n",
        "\n",
        "\n",
        "            v_to_o_edges[1].append(rindex(objects,o_dict[ent2])) # Si usa l'indice dell'oggetto presente tra gli ultimi aggiunti\n",
        "            v_to_o_attr.append(r_dict[rel])\n",
        "\n",
        "        else:\n",
        "          # controllare se oggetto è gia presente nei current objects,\n",
        "          # in caso contrario si aggiunge.\n",
        "          # Non dovrebbe esserci più oggetti dello stesso tipo nel grafo in cui si connettono tra loro oggetti\n",
        "            if o_dict[ent1] not in current_graph_objects.keys():\n",
        "                o_count = o_count + 1\n",
        "                current_graph_objects[o_dict[ent1]] = len(objects)\n",
        "                objects.append(o_dict[ent1])\n",
        "            o_to_o_edges[0].append(current_graph_objects[o_dict[ent1]])\n",
        "            if o_dict[ent2] not in current_graph_objects.keys():\n",
        "                o_count = o_count + 1\n",
        "                current_graph_objects[o_dict[ent2]] = len(objects)\n",
        "                objects.append(o_dict[ent2])\n",
        "            o_to_o_edges[1].append(current_graph_objects[o_dict[ent2]])\n",
        "            o_to_o_attr.append(r_dict[rel])\n",
        "      else:\n",
        "        if current_verb == \"\":\n",
        "          current_verb = v_dict[ent2]\n",
        "          verbs.append(current_verb)\n",
        "          verb_uids.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "        elif current_verb != v_dict[ent2]:\n",
        "          print(f\"Error b: two verbs present {ent1} {ent2} {current_verb}\")\n",
        "          print(g['graph_uid'])\n",
        "          sys.exit()\n",
        "    # Features\n",
        "    for j in range(0, o_count):\n",
        "      index = len(objects_features)\n",
        "      objects_features = torch.cat((objects_features, torch.zeros(num_obj_features).unsqueeze(0)), dim=0)\n",
        "      objects_features[index, 0] = objects[j]\n",
        "    i = 1\n",
        "    groundings = g['groundings']\n",
        "    for frame in groundings:\n",
        "        for key in groundings[frame].keys():\n",
        "            values = list(groundings[frame][key].values())\n",
        "            values = [float(v) for v in values]\n",
        "            values = torch.tensor(values, dtype = torch.float)\n",
        "            index = -1\n",
        "            if \":1\" in key and duplicate_index != -1:\n",
        "                index = duplicate_index\n",
        "                key = key[:len(key)-2]\n",
        "            else:\n",
        "                if key in o_dict.keys() and o_dict[key] in current_graph_objects.keys():\n",
        "                    index = current_graph_objects[o_dict[key]]\n",
        "            if index != -1:\n",
        "                for j in range(0, 4):\n",
        "                    objects_features[index, j + i] = values[j]\n",
        "            else:\n",
        "              if o_dict[key] not in current_seen_objects.keys():\n",
        "                CW_to_seen[0].append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "                CW_to_seen[1].append(len(seen_objects_features))\n",
        "                current_seen_objects[o_dict[key]] = len(seen_objects_features)\n",
        "                index = current_seen_objects[o_dict[key]]\n",
        "                seen_objects_features = torch.cat((seen_objects_features, torch.zeros(num_obj_features).unsqueeze(0)), dim=0)\n",
        "                seen_objects_features[-1][0] = o_dict[key]\n",
        "                seen_obj_uid.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "              else:\n",
        "                index = current_seen_objects[o_dict[key]]\n",
        "              for j in range(0, 4):\n",
        "                    seen_objects_features[index, j + i] = values[j]\n",
        "        i = i + 4\n",
        "    #ID assignment\n",
        "    for j in range(0, o_count):\n",
        "       graph_index_list.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "    #Time edges\n",
        "    if previous_graph_objects != {}:\n",
        "        for key in previous_graph_objects.keys():\n",
        "            if key in current_graph_objects.keys():\n",
        "                time_edges[0].append(previous_graph_objects[key])\n",
        "                time_edges[1].append(current_graph_objects[key])\n",
        "    previous_graph_objects = current_graph_objects\n",
        "    current_graph_objects = {}\n",
        "    current_seen_objects = {}\n",
        "  #v to v\n",
        "  v_to_v = [[], []]\n",
        "  for i in range(len(verbs) - 1):\n",
        "    v_to_v[0].append(i)\n",
        "    v_to_v[1].append(i + 1)\n",
        "  # Verbs features\n",
        "  verbs_features = torch.tensor(verbs, dtype=torch.float)\n",
        "  if include_extracted_features:\n",
        "    verbs_features = torch.empty(0, num_verb_features, dtype=torch.float)\n",
        "    for i in range(len(verbs)):\n",
        "      t = torch.cat((torch.tensor([verbs[i]]), verb_feats[extra_features['graph_uids'][verb_uids[i]]]))\n",
        "      verbs_features = torch.cat((verbs_features, t.unsqueeze(0)), dim=0)\n",
        "  # Seen objects\n",
        "  s_index = len(objects_features)\n",
        "  objects_features = torch.cat((objects_features, seen_objects_features), dim=0)\n",
        "  obj_graph_uids = torch.tensor(graph_index_list + seen_obj_uid, dtype=torch.int64)\n",
        "  for i in range(len(CW_to_seen[0])):\n",
        "    CW_to_seen[1][i] = CW_to_seen[1][i] + s_index\n",
        "\n",
        "  dictionary = {}\n",
        "  if include_extracted_features:\n",
        "    dictionary['verbs_features'] = verbs_features\n",
        "  else:\n",
        "    dictionary['verbs_features'] = verbs_features.unsqueeze(1)\n",
        "  dictionary['obj_features'] = objects_features\n",
        "  # dictionary['seen_obj_features'] = seen_objects_features\n",
        "  dictionary['v_to_o'] = torch.tensor(v_to_o_edges, dtype=torch.int64)\n",
        "  dictionary['v_to_o_attr'] = torch.tensor(v_to_o_attr, dtype=torch.float)\n",
        "  dictionary['dobj_edge'] = torch.tensor(dobj_edge, dtype=torch.int64)\n",
        "  dictionary['o_to_o'] = torch.tensor(o_to_o_edges, dtype=torch.int64)\n",
        "  dictionary['o_to_o_attr'] = torch.tensor(o_to_o_attr, dtype=torch.float)\n",
        "  dictionary['time_edges'] = torch.tensor(time_edges, dtype=torch.int64)\n",
        "  dictionary['v_to_v'] = torch.tensor(v_to_v, dtype=torch.int64)\n",
        "  dictionary['CW_to_seen'] = torch.tensor(CW_to_seen, dtype=torch.int64)\n",
        "  dictionary['obj_graph_uids'] = obj_graph_uids\n",
        "  dictionary['verb_graph_uids'] = torch.tensor(verb_uids, dtype=torch.int64)\n",
        "  # dictionary['seen_obj_graph_uids'] = torch.tensor(seen_obj_uid, dtype=torch.int64)\n",
        "  dictionary['CW_graph_uids'] = torch.tensor(CW_uids, dtype=torch.int64)\n",
        "  dictionary['extra'] = extra_features\n",
        "\n",
        "  return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsGVE-07pest"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "def createHeteroData(source):\n",
        "  data = HeteroData()\n",
        "\n",
        "  data['verb'].x = source['verbs_features']\n",
        "  data['object'].x = source['obj_features']\n",
        "  # data['seen_object'].x = source['seen_obj_features']\n",
        "  data['CW'].num_nodes = len(source['verbs_features'])\n",
        "\n",
        "  data['verb', 'rel', 'object'].edge_index = source['v_to_o']\n",
        "  data['verb', 'rel', 'object'].edge_attr = source['v_to_o_attr']\n",
        "  data['verb', 'dobj', 'object'].edge_index = source['dobj_edge']\n",
        "  data['object', 'rel', 'object'].edge_index = source['o_to_o']\n",
        "  data['object', 'rel', 'object'].edge_attr = source['o_to_o_attr']\n",
        "  data['object', 'time', 'object'].edge_index = source['time_edges']\n",
        "  data['verb', 'next', 'verb'].edge_index = source['v_to_v']\n",
        "  data['CW', 'sees', 'object'].edge_index = source['CW_to_seen']\n",
        "\n",
        "  data['object'].extra_features = source['obj_graph_uids']\n",
        "  data['verb'].extra_features = source['verb_graph_uids']\n",
        "  # data['seen_object'].extra_features = source['seen_obj_graph_uids']\n",
        "  data['CW'].extra_features = source['CW_graph_uids']\n",
        "\n",
        "  data.extra_features = source['extra']\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QDQvbIbcJVt"
      },
      "outputs": [],
      "source": [
        "def from_json_to_heteroData_list(data):\n",
        "  var = 0\n",
        "  data_list = []\n",
        "  for d in data.keys():\n",
        "    graph_dict = extract_from_sequence(data[d], include_extracted_features)\n",
        "    if var == 0:\n",
        "      var += 1\n",
        "    h_data = createHeteroData(graph_dict)\n",
        "    if var == 1:\n",
        "      var = 2\n",
        "    data_list.append(h_data)\n",
        "  return data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzuIM_I0np9Z"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def extract_subgraph(original, span, include_future_object = False, word2vec = False):\n",
        "  label = {'verb': -1, 'object': -1}\n",
        "  target_index = span[-1] + 1\n",
        "  verb_idx = torch.nonzero(create_mask(original['verb'].extra_features, [target_index]))\n",
        "  label['verb'] = int(original['verb'].x[int(verb_idx)][0])\n",
        "  for index in range(len(original['verb', 'dobj', 'object'].edge_index[0])):\n",
        "    if original['verb', 'dobj', 'object'].edge_index[0][index] == verb_idx:\n",
        "      label['object'] = int(original['object'].x[original['verb', 'dobj', 'object'].edge_index[1][index]][0])\n",
        "      break\n",
        "  \"\"\"\n",
        "  label_objects = torch.nonzero(create_mask(original['object'].extra_features, [target_index]))\n",
        "  for index in range(len(original['verb', 'rel', 'object'].edge_index[0])):\n",
        "    if original['verb', 'rel', 'object'].edge_index[1][index] in label_objects and original['verb', 'rel', 'object'].edge_attr[index] == r_dict['dobj']:\n",
        "      label['object'] = int(original['object'].x[original['verb', 'rel', 'object'].edge_index[1][index]][0])\n",
        "      break\"\"\"\n",
        "  if(label['object'] == -1):\n",
        "    print(\"NO LABEL\")\n",
        "    sys.exit()\n",
        "  masks = {}\n",
        "  for node_type in original.node_types:\n",
        "    masks[node_type] = create_mask(original[node_type].extra_features, span)\n",
        "  subgraph_data = original.subgraph(masks)\n",
        "  subgraph_data.extra_features = original.extra_features.copy()\n",
        "  subgraph_data.extra_features['graph_uids'] = copy.deepcopy(original.extra_features['graph_uids'][span[0]:span[0]+len(span)])\n",
        "  subgraph_data.extra_features['label_uid'] = original.extra_features['graph_uids'][target_index]\n",
        "  inv_mask = ~masks['object']\n",
        "\n",
        "  if not include_future_object:\n",
        "    inv_mask[int(torch.nonzero(masks['object'])[0]):] = False\n",
        "  other_obj = original['object'].x[inv_mask]\n",
        "  other_obj_extra = original['object'].extra_features[inv_mask]\n",
        "  other_edges = [[],[]]\n",
        "  s_index = len(subgraph_data['object'].x)\n",
        "  subgraph_data['object'].x = torch.cat((subgraph_data['object'].x, other_obj), dim=0)\n",
        "  subgraph_data['object'].extra_features = torch.cat((subgraph_data['object'].extra_features, other_obj_extra), dim=0)\n",
        "  for i in range(len(other_obj)):\n",
        "    for j in range(len(span)):\n",
        "      other_edges[0].append(i + s_index)\n",
        "      other_edges[1].append(j)\n",
        "  subgraph_data['object', 'has_seen', 'CW'].edge_index = torch.tensor(other_edges, dtype=torch.int64)\n",
        "  other_edges.reverse()\n",
        "  subgraph_data['CW', 'has_seen', 'object'].edge_index = torch.tensor(other_edges, dtype=torch.int64)\n",
        "  subgraph_data.y_verb = torch.tensor([label['verb']], dtype=torch.int64)\n",
        "  subgraph_data.y_obj = torch.tensor([label['object']], dtype=torch.int64)\n",
        "  subgraph_data['CW'].x = torch.zeros(subgraph_data['CW'].num_nodes, 1, dtype=torch.float)\n",
        "\n",
        "  if word2vec:\n",
        "    subgraph_data = create_embeddings(subgraph_data)\n",
        "  return remove_empty(subgraph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmOmpVPJE_2i"
      },
      "outputs": [],
      "source": [
        "def remove_empty(data):\n",
        "  to_remove = []\n",
        "  for node_type in data.node_types:\n",
        "    if 'x' in data[node_type].keys() and data[node_type].x.size(0) == 0:\n",
        "      to_remove.append(node_type)\n",
        "\n",
        "  for edge_typpe in data.edge_types:\n",
        "    if 'edge_index' in data[node_type].keys() and data[node_type].edge_index.size(0) == 0:\n",
        "      to_remove.append(node_type)\n",
        "\n",
        "  for node_type in to_remove:\n",
        "    del data[node_type]\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfKHyxAWndgS"
      },
      "outputs": [],
      "source": [
        "rev_o_dict = {v: k for k, v in o_dict.items()}\n",
        "rev_v_dict = {v: k for k, v in v_dict.items()}\n",
        "rev_r_dict = {v: k for k, v in r_dict.items()}\n",
        "rev_r_dict[r_dict['dobj']] ='direct object'\n",
        "rev_r_dict[r_dict['to']] ='To'\n",
        "rev_v_dict[v_dict['unhang']] = 'not hang'\n",
        "\n",
        "def create_embedding(label):\n",
        "  embeddings= []\n",
        "  for word in label.split():\n",
        "    embeddings.append(torch.tensor(w2v[word]))\n",
        "  final_embedding = []\n",
        "  for embd in embeddings:\n",
        "    if final_embedding == []:\n",
        "      final_embedding = torch.tensor(embd, dtype=torch.float)\n",
        "    else:\n",
        "      final_embedding = torch.mean(torch.stack([final_embedding, embd], dim=0), dim=0)\n",
        "  return final_embedding\n",
        "\n",
        "def create_embeddings(hData):\n",
        "  for node_type, y in hData.node_items():\n",
        "    u_dict = rev_o_dict\n",
        "    if 'x' in y.keys() and node_type != 'CW':\n",
        "      if hData[node_type].x.numel() != 0:\n",
        "        new_x = torch.empty(0, len(hData[node_type].x[0]) + 299, dtype=torch.float)\n",
        "        if node_type == 'verb':\n",
        "          u_dict = rev_v_dict\n",
        "        for i in range(0, len(hData[node_type].x)):\n",
        "          obj_embedding = create_embedding(u_dict[int(hData[node_type].x[i][0])])\n",
        "          existing_features = hData[node_type].x[i][1:]\n",
        "          obj = torch.cat((obj_embedding, existing_features), dim=0)\n",
        "          new_x = torch.cat((new_x, obj.unsqueeze(0)), dim=0)\n",
        "        hData[node_type].x = new_x\n",
        "  for edge_type, y in hData.edge_items():\n",
        "    if 'edge_attr' in y.keys() and len(hData[edge_type].edge_attr) >0:\n",
        "      original_t = hData[edge_type].edge_attr\n",
        "      embeddings = []\n",
        "      for i in range(len(hData[edge_type].edge_attr)):\n",
        "        embeddings.append(create_embedding(rev_r_dict[int(hData[edge_type].edge_attr[i])]))\n",
        "      hData[edge_type].edge_attr = torch.stack(embeddings)\n",
        "  return hData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ZbFYGTrFyf"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Dataset\n",
        "\n",
        "class HeteroDataset(Dataset):\n",
        "    def __init__(self, data_list):\n",
        "        super().__init__()\n",
        "        self.data_list = data_list\n",
        "        self._indices = None\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data_list[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXn-qs6ezKPC"
      },
      "outputs": [],
      "source": [
        "def create_samples(data_obj_list, seq_length):\n",
        "  data_list = []\n",
        "  id_lists = []\n",
        "  for hData in data_obj_list:\n",
        "    if len(hData.extra_features[\"graph_uids\"]) > seq_length:\n",
        "      id_lists.append(hData.extra_features[\"video_uid\"])\n",
        "      for i in range(len(hData.extra_features[\"graph_uids\"]) - seq_length):\n",
        "        data_list.append(extract_subgraph(hData, range(i, i + seq_length), include_future_object, word2vec=word2vec))\n",
        "  return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlLlU4paRGl6"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiK7EYGDQ6A1"
      },
      "outputs": [],
      "source": [
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i00bNJooA_-g"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBm_hrDZ2x0F",
        "outputId": "95ee8d42-a0c6-493c-ae16-9d06a563ea24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The active device is cuda\n"
          ]
        }
      ],
      "source": [
        "device ='cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "print(f\"The active device is {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbCVKPcnW8-7"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Q_ienGCrDr"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE5F-5F6gem9"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Linear, to_hetero\n",
        "from torch.nn import Softmax, Parameter\n",
        "\n",
        "class GNNWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        model = GNN(hidden_channels, hidden_channels)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "        self.linear_verb = Linear(hidden_channels, out_channels[0])\n",
        "        self.linear_obj = Linear(hidden_channels, out_channels[1])\n",
        "\n",
        "        self.alpha = Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "        self.beta = Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gnn(x, edge_index)\n",
        "\n",
        "        x_verb = self.linear_verb(x['verb'])\n",
        "        x_obj = self.linear_obj(x['object'])\n",
        "\n",
        "        return x_verb, x_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAGEConv"
      ],
      "metadata": {
        "id": "x8szjkN8LBN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import Linear, to_hetero, SAGEConv\n",
        "from torch.nn import Softmax, Parameter\n",
        "import torch.nn.init as init\n",
        "\n",
        "class SAGEConvGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv3 = SAGEConv((-1, -1), out_channels)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class SAGEWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        model = SAGEConvGNN(hidden_channels, hidden_channels)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "\n",
        "        self.lin1 = Linear(-1, hidden_channels)\n",
        "        self.lin2 = Linear(-1, hidden_channels)\n",
        "        self.lin3 = Linear(-1, hidden_channels)\n",
        "\n",
        "        self.linear_verb = Linear(-1, out_channels[0])\n",
        "        self.linear_obj = Linear(-1, out_channels[1])\n",
        "\n",
        "        self.theta = Parameter(torch.randn(1))\n",
        "        self.register_buffer('alpha', torch.zeros(1))\n",
        "        self.register_buffer('beta', torch.zeros(1))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        self.beta = torch.sigmoid(self.theta)\n",
        "        self.alpha = 1 - self.beta\n",
        "\n",
        "        x = self.gnn(x, edge_index)\n",
        "\n",
        "        x_verb = self.lin1(x['verb']).relu()\n",
        "        x_verb = self.lin2(x_verb).relu()\n",
        "        x_verb = self.lin3(x_verb).relu()\n",
        "        x_verb = self.linear_verb(x_verb)\n",
        "\n",
        "        x_obj = self.lin1(x['object']).relu()\n",
        "        x_obj = self.lin2(x_obj).relu()\n",
        "        x_obj = self.lin3(x_obj).relu()\n",
        "        x_obj = self.linear_obj(x_obj)\n",
        "\n",
        "        return x_verb, x_obj"
      ],
      "metadata": {
        "id": "WwW_xCtmLApZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwA14VH1CzOY"
      },
      "source": [
        "### GAT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZh9oML0Ff6Y"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAT\n",
        "from torch_geometric.nn import Linear, to_hetero\n",
        "from torch.nn import Softmax, Parameter\n",
        "\n",
        "\n",
        "class GATWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        model = GAT((-1, -1), hidden_channels, out_channels=hidden_channels, num_layers=num_layers, add_self_loops=False)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
        "        self.linear_verb = Linear(hidden_channels, out_channels[0])\n",
        "        self.linear_obj = Linear(hidden_channels, out_channels[1])\n",
        "\n",
        "        self.theta = Parameter(torch.randn(1))\n",
        "        self.register_buffer('alpha', torch.zeros(1))\n",
        "        self.register_buffer('beta', torch.zeros(1))\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        self.beta = torch.sigmoid(self.theta)\n",
        "        self.alpha = 1 - self.beta\n",
        "\n",
        "        if edge_attr == None:\n",
        "          x = self.gnn(x, edge_index)\n",
        "        else:\n",
        "          x = self.gnn(x, edge_index, edge_attr)\n",
        "\n",
        "        x_verb = self.lin1(x['verb']).relu()\n",
        "        x_verb = self.lin2(x_verb).relu()\n",
        "        x_verb = self.lin3(x_verb).relu()\n",
        "        x_verb = self.linear_verb(x_verb)\n",
        "\n",
        "        x_obj = self.lin1(x['object']).relu()\n",
        "        x_obj = self.lin2(x_obj).relu()\n",
        "        x_obj = self.lin3(x_obj).relu()\n",
        "        x_obj = self.linear_obj(x_obj)\n",
        "\n",
        "        return x_verb, x_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87NNXoZqC3RK"
      },
      "source": [
        "### Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpYetD3wcL4S"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Linear, to_hetero, TransformerConv\n",
        "from torch.nn import Softmax, Parameter\n",
        "import torch.nn.init as init\n",
        "\n",
        "class TransformerConvGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = TransformerConv((-1, -1), hidden_channels, heads=num_layers)\n",
        "        self.conv2 = TransformerConv((-1, -1), hidden_channels, heads=num_layers)\n",
        "        self.conv3 = TransformerConv((-1, -1), out_channels, heads=num_layers)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class TransformerWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        model = TransformerConvGNN(hidden_channels, hidden_channels)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "\n",
        "        self.lin1 = Linear(-1, hidden_channels)\n",
        "        self.lin2 = Linear(-1, hidden_channels)\n",
        "        self.lin3 = Linear(-1, hidden_channels)\n",
        "\n",
        "        self.linear_verb = Linear(-1, out_channels[0])\n",
        "        self.linear_obj = Linear(-1, out_channels[1])\n",
        "\n",
        "        self.theta = Parameter(torch.randn(1))\n",
        "        self.register_buffer('alpha', torch.zeros(1))\n",
        "        self.register_buffer('beta', torch.zeros(1))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        self.beta = torch.sigmoid(self.theta)\n",
        "        self.alpha = 1 - self.beta\n",
        "\n",
        "        x = self.gnn(x, edge_index)\n",
        "\n",
        "        x_verb = self.lin1(x['verb']).relu()\n",
        "        x_verb = self.lin2(x_verb).relu()\n",
        "        x_verb = self.lin3(x_verb).relu()\n",
        "        x_verb = self.linear_verb(x_verb)\n",
        "\n",
        "        x_obj = self.lin1(x['object']).relu()\n",
        "        x_obj = self.lin2(x_obj).relu()\n",
        "        x_obj = self.lin3(x_obj).relu()\n",
        "        x_obj = self.linear_obj(x_obj)\n",
        "\n",
        "        return x_verb, x_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48c77gbx0_D6"
      },
      "source": [
        "## Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9_W28MsjB4o"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def process_logits(verb_logits, object_logits, batch_dict):\n",
        "  batches = torch.unique(batch_dict['verb'])\n",
        "  v_temp_list = []\n",
        "  o_temp_list = []\n",
        "  for b in batches:\n",
        "\n",
        "    v_indices = torch.nonzero(batch_dict['verb'] == b, as_tuple=False).squeeze(1)\n",
        "    v_temp = torch.mean(verb_logits[v_indices], dim=0, keepdim=True)\n",
        "    v_temp_list.append(v_temp)\n",
        "\n",
        "    o_indices = torch.nonzero(batch_dict['object'] == b, as_tuple=False).squeeze(1)\n",
        "    o_temp = torch.mean(object_logits[o_indices], dim=0, keepdim=True)\n",
        "    o_temp_list.append(o_temp)\n",
        "\n",
        "  v_temp_matrix = torch.cat(v_temp_list, dim=0)\n",
        "  v_temp_matrix = F.softmax(v_temp_matrix, dim=1)\n",
        "\n",
        "  o_temp_matrix = torch.cat(o_temp_list, dim=0)\n",
        "  o_temp_matrix = F.softmax(o_temp_matrix, dim=1)\n",
        "\n",
        "  return v_temp_matrix, o_temp_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML2xHMvC4xXI"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "def train(model, train_loader, optimizer, scheduler, device = 'cpu', epochs = 10, val_loader = None, extra_features = False, trained_loss = False, loss_weights=(1,1)):\n",
        "    model = model.to(device)\n",
        "\n",
        "    history = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_verb_correct_1 = 0\n",
        "        total_object_correct_1 = 0\n",
        "        total_verb_correct_5 = 0\n",
        "        total_object_correct_5 = 0\n",
        "        total_action_correct_1 = 0\n",
        "        total_action_correct_5 = 0\n",
        "        total_verb_loss = 0\n",
        "        total_object_loss = 0\n",
        "\n",
        "        for data in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if extra_features:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict)\n",
        "            else:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict)\n",
        "            n_verb_logits, n_object_logits = process_logits(verb_logits, object_logits, data.batch_dict)\n",
        "            loss1 = F.cross_entropy(n_verb_logits, data.y_verb)\n",
        "            loss2 = F.cross_entropy(n_object_logits, data.y_obj)\n",
        "            if trained_loss:\n",
        "              loss = (10 * model.alpha * loss1) + (model.beta * loss2 * 10)\n",
        "            else:\n",
        "              loss = loss_weights[0] * loss1 + loss_weights[1] * loss2\n",
        "            total_verb_loss += loss1.item()\n",
        "            total_object_loss += loss2.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, verb_logits_topk = torch.topk(n_verb_logits, 5, largest=True, sorted=True)\n",
        "            _, object_logits_topk = torch.topk(n_object_logits, 5, largest=True, sorted=True)\n",
        "\n",
        "            total_verb_correct_1 += (verb_logits_topk[:, 0] == data.y_verb).sum().item()\n",
        "            total_object_correct_1 += (object_logits_topk[:, 0] == data.y_obj).sum().item()\n",
        "            total_action_correct_1 += ((verb_logits_topk[:, 0] == data.y_verb) & (object_logits_topk[:, 0] == data.y_obj)).sum().item()\n",
        "\n",
        "            total_verb_correct_5 += sum([data.y_verb[i] in verb_logits_topk[i] for i in range(data.y_verb.size(0))])\n",
        "            total_object_correct_5 += sum([data.y_obj[i] in object_logits_topk[i] for i in range(data.y_obj.size(0))])\n",
        "            total_action_correct_5 += sum([(data.y_verb[i] in verb_logits_topk[i]) & (data.y_obj[i] in object_logits_topk[i]) for i in range(data.y_obj.size(0))])\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataset)\n",
        "        train_metrics = {\n",
        "            'lr': optimizer.param_groups[0]['lr'],\n",
        "            'train_loss': avg_loss,\n",
        "            'train_verb_loss': total_verb_loss,\n",
        "            'train_obj_loss': total_object_loss,\n",
        "            'train_v_acc_1': total_verb_correct_1 / len(train_dataset),\n",
        "            'train_o_acc_1': total_object_correct_1 / len(train_dataset),\n",
        "            'train_a_acc_1': total_action_correct_1 / len(train_dataset),\n",
        "            'train_v_acc_5': total_verb_correct_5 / len(train_dataset),\n",
        "            'train_o_acc_5': total_object_correct_5 / len(train_dataset),\n",
        "            'train_a_acc_5': total_action_correct_5 / len(train_dataset)\n",
        "        }\n",
        "        if trained_loss:\n",
        "          train_metrics['loss alpha'] = model.alpha.item()\n",
        "          train_metrics['loss beta'] = model.beta.item()\n",
        "        if val_loader is not None:\n",
        "          val_metrics = validate(model, val_loader, device, extra_features, trained_loss, loss_weights)\n",
        "\n",
        "        history.append({**train_metrics, **val_metrics})\n",
        "        wandb.log({**train_metrics, **val_metrics})\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {avg_loss:.4f}, \"\n",
        "              f\"Verb Acc@1: {train_metrics['train_v_acc_1']*100:.2f}%, \"\n",
        "              f\"Object Acc@1: {train_metrics['train_o_acc_1']*100:.2f}%, \"\n",
        "              f\"Action Acc@1: {train_metrics['train_a_acc_1']*100:.2f}%, \"\n",
        "              f\"Verb Acc@5: {train_metrics['train_v_acc_5']*100:.2f}%, \"\n",
        "              f\"Object Acc@5: {train_metrics['train_o_acc_5']*100:.2f}%, \"\n",
        "              f\"Action Acc@5: {train_metrics['train_a_acc_5']*100:.2f}%\")\n",
        "        if val_dataset is not None:\n",
        "          print(f\"Validation Loss: {val_metrics['val_loss']:.4f}, \"\n",
        "                f\"Verb Acc@1: {val_metrics['val_v_acc_1']*100:.2f}%, \"\n",
        "                f\"Object Acc@1: {val_metrics['val_o_acc_1']*100:.2f}%, \"\n",
        "                f\"Action Acc@1: {val_metrics['val_a_acc_1']*100:.2f}%, \"\n",
        "                f\"Verb Acc@5: {val_metrics['val_v_acc_5']*100:.2f}%, \"\n",
        "                f\"Object Acc@5: {val_metrics['val_o_acc_5']*100:.2f}%, \"\n",
        "                f\"Action Acc@5: {val_metrics['val_a_acc_5']*100:.2f}%\")\n",
        "          print(f\"Loss alpha: {model.alpha},\"\n",
        "                f\"Loss beta: {model.beta}\")\n",
        "    return history\n",
        "\n",
        "def validate(model, val_loader, device, extra_features=False, trained_loss = False, loss_weights=(1,1)):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_verb_correct_1 = 0\n",
        "    total_object_correct_1 = 0\n",
        "    total_verb_correct_5 = 0\n",
        "    total_object_correct_5 = 0\n",
        "    total_action_correct_1 = 0\n",
        "    total_action_correct_5 = 0\n",
        "    total_verb_loss = 0\n",
        "    total_object_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(val_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "\n",
        "            if extra_features:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict)\n",
        "            else:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict)\n",
        "            n_verb_logits, n_object_logits = process_logits(verb_logits, object_logits, data.batch_dict)\n",
        "            loss1 = F.cross_entropy(n_verb_logits, data.y_verb)\n",
        "            loss2 = F.cross_entropy(n_object_logits, data.y_obj)\n",
        "            if trained_loss:\n",
        "              loss = (10 * model.alpha * loss1) + (model.beta * loss2 * 10)\n",
        "            else:\n",
        "              loss = loss_weights[0] * loss1 + loss_weights[1] * loss2\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, verb_logits_topk = torch.topk(n_verb_logits, 5, largest=True, sorted=True)\n",
        "            _, object_logits_topk = torch.topk(n_object_logits, 5, largest=True, sorted=True)\n",
        "\n",
        "            total_verb_correct_1 += (verb_logits_topk[:, 0] == data.y_verb).sum().item()\n",
        "            total_object_correct_1 += (object_logits_topk[:, 0] == data.y_obj).sum().item()\n",
        "            total_action_correct_1 += ((verb_logits_topk[:, 0] == data.y_verb) & (object_logits_topk[:, 0] == data.y_obj)).sum().item()\n",
        "\n",
        "            total_verb_correct_5 += sum([data.y_verb[i] in verb_logits_topk[i] for i in range(data.y_verb.size(0))])\n",
        "            total_object_correct_5 += sum([data.y_obj[i] in object_logits_topk[i] for i in range(data.y_obj.size(0))])\n",
        "            total_action_correct_5 += sum([(data.y_verb[i] in verb_logits_topk[i]) & (data.y_obj[i] in object_logits_topk[i]) for i in range(data.y_obj.size(0))])\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataset)\n",
        "    val_metrics = {\n",
        "        'val_loss': avg_loss,\n",
        "        'val_verb_loss': total_verb_loss,\n",
        "        'val_obj_loss': total_object_loss,\n",
        "        'val_v_acc_1': total_verb_correct_1 / len(val_dataset),\n",
        "        'val_o_acc_1': total_object_correct_1 / len(val_dataset),\n",
        "        'val_a_acc_1': total_action_correct_1 / len(val_dataset),\n",
        "        'val_v_acc_5': total_verb_correct_5 / len(val_dataset),\n",
        "        'val_o_acc_5': total_object_correct_5 / len(val_dataset),\n",
        "        'val_a_acc_5': total_action_correct_5 / len(val_dataset)\n",
        "    }\n",
        "\n",
        "    return val_metrics\n",
        "\n",
        "def test(model, test_loader, device, extra_features=False, trained_loss = False, loss_weights=(1,1)):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_verb_correct_1 = 0\n",
        "    total_object_correct_1 = 0\n",
        "    total_verb_correct_5 = 0\n",
        "    total_object_correct_5 = 0\n",
        "    total_action_correct_1 = 0\n",
        "    total_action_correct_5 = 0\n",
        "    total_verb_loss = 0\n",
        "    total_object_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(val_loader, desc=\"Testing\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "\n",
        "            if extra_features:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict)\n",
        "            else:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict)\n",
        "            n_verb_logits, n_object_logits = process_logits(verb_logits, object_logits, data.batch_dict)\n",
        "            loss1 = F.cross_entropy(n_verb_logits, data.y_verb)\n",
        "            loss2 = F.cross_entropy(n_object_logits, data.y_obj)\n",
        "            if trained_loss:\n",
        "              loss = (10 * model.alpha * loss1) + (model.beta * loss2 * 10)\n",
        "            else:\n",
        "              loss = loss_weights[0] * loss1 + loss_weights[1] * loss2\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, verb_logits_topk = torch.topk(n_verb_logits, 5, largest=True, sorted=True)\n",
        "            _, object_logits_topk = torch.topk(n_object_logits, 5, largest=True, sorted=True)\n",
        "\n",
        "            total_verb_correct_1 += (verb_logits_topk[:, 0] == data.y_verb).sum().item()\n",
        "            total_object_correct_1 += (object_logits_topk[:, 0] == data.y_obj).sum().item()\n",
        "            total_action_correct_1 += ((verb_logits_topk[:, 0] == data.y_verb) & (object_logits_topk[:, 0] == data.y_obj)).sum().item()\n",
        "\n",
        "            total_verb_correct_5 += sum([data.y_verb[i] in verb_logits_topk[i] for i in range(data.y_verb.size(0))])\n",
        "            total_object_correct_5 += sum([data.y_obj[i] in object_logits_topk[i] for i in range(data.y_obj.size(0))])\n",
        "            total_action_correct_5 += sum([(data.y_verb[i] in verb_logits_topk[i]) & (data.y_obj[i] in object_logits_topk[i]) for i in range(data.y_obj.size(0))])\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataset)\n",
        "    test_metrics = {\n",
        "        'test_loss': avg_loss,\n",
        "        'test_verb_loss': total_verb_loss,\n",
        "        'test_obj_loss': total_object_loss,\n",
        "        'test_v_acc_1': total_verb_correct_1 / len(val_dataset),\n",
        "        'test_o_acc_1': total_object_correct_1 / len(val_dataset),\n",
        "        'test_a_acc_1': total_action_correct_1 / len(val_dataset),\n",
        "        'test_v_acc_5': total_verb_correct_5 / len(val_dataset),\n",
        "        'test_o_acc_5': total_object_correct_5 / len(val_dataset),\n",
        "        'test_a_acc_5': total_action_correct_5 / len(val_dataset)\n",
        "    }\n",
        "    print(f\"Validation Loss: {test_metrics['test_loss']:.4f}, \"\n",
        "                f\"Verb Acc@1: {test_metrics['test_v_acc_1']*100:.2f}%, \"\n",
        "                f\"Object Acc@1: {test_metrics['test_o_acc_1']*100:.2f}%, \"\n",
        "                f\"Action Acc@1: {test_metrics['test_a_acc_1']*100:.2f}%, \"\n",
        "                f\"Verb Acc@5: {test_metrics['test_v_acc_5']*100:.2f}%, \"\n",
        "                f\"Object Acc@5: {test_metrics['test_o_acc_5']*100:.2f}%, \"\n",
        "                f\"Action Acc@5: {test_metrics['test_a_acc_5']*100:.2f}%\")\n",
        "    wandb.log({**test_metrics})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "wL8pRWJ2759J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline"
      ],
      "metadata": {
        "id": "itUQ2skn8WtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "HlQ9d5FS744u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No extra"
      ],
      "metadata": {
        "id": "9gd2dOqI8mcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= True\n",
        "include_extracted_features = False #<----------\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "AzXEvhM18pc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No Word2Vec"
      ],
      "metadata": {
        "id": "4aKd1XBt84g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= False #<----------\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "35h40cvV9Amy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No future object"
      ],
      "metadata": {
        "id": "72s8UxKj9FAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = False #<----------\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "Dha-PNYE9JRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No trained loss"
      ],
      "metadata": {
        "id": "Nh1fafIO-iMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = False #<----------\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = False\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "LVbVr9Of-lr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WKVddwtRKZ6"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP1l7xgR1lTi"
      },
      "source": [
        "### Working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpbtKhU0QrZQ"
      },
      "outputs": [],
      "source": [
        "data_obj_list = from_json_to_heteroData_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ooUvHpMwD_C",
        "outputId": "c65852d5-47ee-4785-83f4-772b4a212979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3029 sequences available from 106 valid scenes. \n",
            "115 non valid scens for a sequence length of 20\n"
          ]
        }
      ],
      "source": [
        "non_valid, valid, tot = calc_possible_seq(seq_length, data_obj_list)\n",
        "print(f\"{tot} sequences available from {valid} valid scenes. \\n{non_valid} non valid scens for a sequence length of {seq_length}\")\n",
        "dataset = HeteroDataset(create_samples(data_obj_list, seq_length))\n",
        "metadata = dataset[0].metadata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzk4LUR8recZ",
        "outputId": "3a0e9926-0120-421e-ec66-f9815c683e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The seed is: 3260\n",
            "Training set size: 2100\n",
            "Validation set size: 450\n",
            "Testing set size: 450\n"
          ]
        }
      ],
      "source": [
        "seed = random.randint(0, 10000)\n",
        "print(f\"The seed is: {seed}\")\n",
        "random.seed(seed)\n",
        "\n",
        "subset = random.sample(dataset.data_list, samples)\n",
        "\n",
        "random.shuffle(subset)\n",
        "\n",
        "num_train = int(0.70 * len(subset))  # 70% for training\n",
        "num_val = int(0.15 * len(subset))    # 15% for validation\n",
        "num_test = len(subset) - num_train - num_val  # 15% for testing\n",
        "\n",
        "train_data = subset[:num_train]\n",
        "val_data = subset[num_train:num_train + num_val]\n",
        "test_data = subset[num_train + num_val:]\n",
        "\n",
        "train_dataset = HeteroDataset(train_data)\n",
        "val_dataset = HeteroDataset(val_data)\n",
        "test_dataset = HeteroDataset(test_data)\n",
        "\n",
        "# Print sizes to verify\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Testing set size: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzuAeIpf-Dv3"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJsEdw425-2"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch = batch.to(device)\n",
        "model = TransformerWithClassifier(metadata, hidden_channels, out_channels).to(device)\n",
        "a, b = model(batch.x_dict, batch.edge_index_dict)\n",
        "if init_weights:\n",
        "  init_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvbp_VGU7Vr-"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "total_epochs = epochs\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch + 1 < epoch_max:\n",
        "        return epoch / epoch_max\n",
        "    else:\n",
        "        return (total_epochs - epoch) / (total_epochs - epoch_max)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "213337b0220440f3adcc3126e84d4a03",
            "88719fb8261648a2a2eb73e131cc8117",
            "951b3f1adaf045f1a0cb02bc55d395e3",
            "27ec815b418f4c1586e4f8b5166e62cd",
            "83d276e668ba46018146e7cd0dc4daa7",
            "d0eb2dbf4d1c4d98b700af28df62f85a",
            "e14741fe489a4006a38572d9cae8f0cf",
            "b710c2693ded4a968221515a37c2bfc5"
          ]
        },
        "id": "_I4RUTvB1RN5",
        "outputId": "3ea26e4e-583d-47cd-f5cc-609b1069fcbe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240710_102357-47jhgy7l</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/unimib-action-prediction/EASG%20GNN/runs/47jhgy7l' target=\"_blank\">lunar-bird-30</a></strong> to <a href='https://wandb.ai/unimib-action-prediction/EASG%20GNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/unimib-action-prediction/EASG%20GNN' target=\"_blank\">https://wandb.ai/unimib-action-prediction/EASG%20GNN</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/unimib-action-prediction/EASG%20GNN/runs/47jhgy7l' target=\"_blank\">https://wandb.ai/unimib-action-prediction/EASG%20GNN/runs/47jhgy7l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.76batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.59batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.7791, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.57%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7791, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.00%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.74batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.71batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Train Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.57%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.22%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.73batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.65batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Train Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.67%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.22%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.61batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Train Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.71%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.22%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.70batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.56batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Train Loss: 1.7792, Verb Acc@1: 0.14%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.90%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 0.44%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.50batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Train Loss: 1.7792, Verb Acc@1: 0.14%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 1.86%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7792, Verb Acc@1: 0.00%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 5.56%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.70batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.60batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Train Loss: 1.7792, Verb Acc@1: 3.43%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 10.90%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7792, Verb Acc@1: 11.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 15.33%, Object Acc@5: 0.00%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.65batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Train Loss: 1.7792, Verb Acc@1: 13.14%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 14.86%, Object Acc@5: 0.10%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7792, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 16.22%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:04<00:00, 11.03batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Train Loss: 1.7792, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 20.19%, Object Acc@5: 0.19%, Action Acc@5: 0.00%\n",
            "Validation Loss: 1.7791, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 21.78%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.47batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Train Loss: 1.7745, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 31.38%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.68batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.61batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.59batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.72batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.62batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.61batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.56batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.71batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.73batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.54batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.72batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.57batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 210/210 [00:36<00:00,  5.71batch/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:03<00:00, 12.60batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20, Train Loss: 1.7659, Verb Acc@1: 14.05%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 34.29%, Object Acc@5: 0.19%, Action Acc@5: 0.14%\n",
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n",
            "Loss alpha: tensor([0.3126], device='cuda:0'),Loss beta: tensor([0.6874], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 45/45 [00:03<00:00, 12.63batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.7644, Verb Acc@1: 15.56%, Object Acc@1: 0.00%, Action Acc@1: 0.00%, Verb Acc@5: 32.44%, Object Acc@5: 0.22%, Action Acc@5: 0.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "213337b0220440f3adcc3126e84d4a03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▂▂▃▄▄▅▅▆█▇▇▆▅▅▄▄▃▂▂</td></tr><tr><td>test_a_acc_1</td><td>▁</td></tr><tr><td>test_a_acc_5</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_o_acc_1</td><td>▁</td></tr><tr><td>test_o_acc_5</td><td>▁</td></tr><tr><td>test_obj_loss</td><td>▁</td></tr><tr><td>test_v_acc_1</td><td>▁</td></tr><tr><td>test_v_acc_5</td><td>▁</td></tr><tr><td>test_verb_loss</td><td>▁</td></tr><tr><td>train_a_acc_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_a_acc_5</td><td>▁▁▁▁▁▁▁▁▁███████████</td></tr><tr><td>train_loss</td><td>█████████▆▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_o_acc_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_o_acc_5</td><td>▁▁▁▁▁▁▁▅████████████</td></tr><tr><td>train_obj_loss</td><td>▁███████████████████</td></tr><tr><td>train_v_acc_1</td><td>▁▁▁▁▁▁▃█████████████</td></tr><tr><td>train_v_acc_5</td><td>▁▁▁▁▁▁▃▄▅▇██████████</td></tr><tr><td>train_verb_loss</td><td>█████████▆▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_a_acc_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_a_acc_5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_o_acc_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_o_acc_5</td><td>▁▁▁▁▁▁▁█████████████</td></tr><tr><td>val_obj_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_v_acc_1</td><td>▁▁▁▁▁▁▆█████████████</td></tr><tr><td>val_v_acc_5</td><td>▁▁▁▁▁▂▄▅▆███████████</td></tr><tr><td>val_verb_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.001</td></tr><tr><td>test_a_acc_1</td><td>0.0</td></tr><tr><td>test_a_acc_5</td><td>0.0</td></tr><tr><td>test_loss</td><td>1.76441</td></tr><tr><td>test_o_acc_1</td><td>0.0</td></tr><tr><td>test_o_acc_5</td><td>0.00222</td></tr><tr><td>test_obj_loss</td><td>0</td></tr><tr><td>test_v_acc_1</td><td>0.15556</td></tr><tr><td>test_v_acc_5</td><td>0.32444</td></tr><tr><td>test_verb_loss</td><td>0</td></tr><tr><td>train_a_acc_1</td><td>0.0</td></tr><tr><td>train_a_acc_5</td><td>0.00143</td></tr><tr><td>train_loss</td><td>1.76592</td></tr><tr><td>train_o_acc_1</td><td>0.0</td></tr><tr><td>train_o_acc_5</td><td>0.0019</td></tr><tr><td>train_obj_loss</td><td>1298.54918</td></tr><tr><td>train_v_acc_1</td><td>0.14048</td></tr><tr><td>train_v_acc_5</td><td>0.34286</td></tr><tr><td>train_verb_loss</td><td>1111.32319</td></tr><tr><td>val_a_acc_1</td><td>0.0</td></tr><tr><td>val_a_acc_5</td><td>0.0</td></tr><tr><td>val_loss</td><td>1.76441</td></tr><tr><td>val_o_acc_1</td><td>0.0</td></tr><tr><td>val_o_acc_5</td><td>0.00222</td></tr><tr><td>val_obj_loss</td><td>0</td></tr><tr><td>val_v_acc_1</td><td>0.15556</td></tr><tr><td>val_v_acc_5</td><td>0.32444</td></tr><tr><td>val_verb_loss</td><td>0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lunar-bird-30</strong> at: <a href='https://wandb.ai/unimib-action-prediction/EASG%20GNN/runs/47jhgy7l' target=\"_blank\">https://wandb.ai/unimib-action-prediction/EASG%20GNN/runs/47jhgy7l</a><br/> View project at: <a href='https://wandb.ai/unimib-action-prediction/EASG%20GNN' target=\"_blank\">https://wandb.ai/unimib-action-prediction/EASG%20GNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240710_102357-47jhgy7l/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "        settings=wandb.Settings(start_method=\"fork\"),\n",
        "        project=\"EASG GNN\",\n",
        "        config={\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"lr\": lr,\n",
        "            \"model_name\": type(model).__name__,\n",
        "            \"sequence_length\": seq_length,\n",
        "            \"extra_features\": extra_features,\n",
        "            \"trained_loss\": trained_loss,\n",
        "            \"init_weights\": init_weights,\n",
        "            \"loss_weights\": loss_weights,\n",
        "            \"word2vec\": word2vec,\n",
        "            \"other objects\": include_future_object,\n",
        "            \"extracted features\": include_extracted_features\n",
        "            })\n",
        "\n",
        "config = wandb.config\n",
        "history = train(model, train_loader, optimizer, scheduler, device, epochs, val_loader=val_loader, extra_features=extra_features, loss_weights=loss_weights)\n",
        "test(model, test_loader, device, extra_features=extra_features, loss_weights=loss_weights)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Wl8KUAHXPv6C",
        "ukL61VIJQEqN",
        "m4FE-ptpP9nN",
        "YLuwzrFMQgSQ",
        "BlLlU4paRGl6",
        "e0Q_ienGCrDr",
        "x8szjkN8LBN5",
        "WwA14VH1CzOY",
        "87NNXoZqC3RK",
        "48c77gbx0_D6",
        "itUQ2skn8WtN",
        "9gd2dOqI8mcZ",
        "4aKd1XBt84g0",
        "72s8UxKj9FAR",
        "Nh1fafIO-iMK"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "213337b0220440f3adcc3126e84d4a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88719fb8261648a2a2eb73e131cc8117",
              "IPY_MODEL_951b3f1adaf045f1a0cb02bc55d395e3"
            ],
            "layout": "IPY_MODEL_27ec815b418f4c1586e4f8b5166e62cd"
          }
        },
        "88719fb8261648a2a2eb73e131cc8117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d276e668ba46018146e7cd0dc4daa7",
            "placeholder": "​",
            "style": "IPY_MODEL_d0eb2dbf4d1c4d98b700af28df62f85a",
            "value": "0.024 MB of 0.024 MB uploaded\r"
          }
        },
        "951b3f1adaf045f1a0cb02bc55d395e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14741fe489a4006a38572d9cae8f0cf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b710c2693ded4a968221515a37c2bfc5",
            "value": 1
          }
        },
        "27ec815b418f4c1586e4f8b5166e62cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83d276e668ba46018146e7cd0dc4daa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0eb2dbf4d1c4d98b700af28df62f85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e14741fe489a4006a38572d9cae8f0cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b710c2693ded4a968221515a37c2bfc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}