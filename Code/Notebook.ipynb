{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr1CcbrkvMx0"
      },
      "source": [
        "# Action prediction on EASG using GNNs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T78NFVBva4s"
      },
      "source": [
        "In this notebook, the groundwork is laid for performing the task of action prediction on EASG. To make the original dataset compatible with PyTorch Geometric, it has been converted into a set of tensors representing nodes and connections. A dictionary has been used to embed verbs, objects, and relationships, converting the word in question into the index of the row where it is located. The annotations are the converted through a Word2Vec model. The graphs are represented as faithfully as possible to the original dataset. To adhere to the methodology proposed in the paper, the graphs representing the scenes are divided into subgraphs representing an adjustable number of consecutive graphs. Some features have been added to maximize the information. Firstly, connections with non-participating objects in the action have been added to the CW node, as they may be part of subsequent actions.\n",
        "\n",
        "Additionally, in creating the subgraphs, a relationship with the objects seen in the graphs preceding the subgraph in question has been included to account for the observer's prior experience. The task in question adheres to what is proposed in the paper, but the model used are radically different from GPT. The original files have been modified by removing non-compliant graphs and adding necessary annotations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl8KUAHXPv6C"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code contains all the imports needed to start running this project."
      ],
      "metadata": {
        "id": "d0puzIKilXsn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycoZ4dam6C1R"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install wandb\n",
        "!pip install torchmetrics\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify this code to the path where your files are stored."
      ],
      "metadata": {
        "id": "BHcrWkjfljeu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0NTR6FCtB8J"
      },
      "outputs": [],
      "source": [
        "base_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3bBhsjE7VQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6587c4c-cacf-41d0-b533-1f1d943481c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/EASG/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CiSZxCYrQ6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf8ca18-4b2f-4916-e3dd-30abb845c3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukL61VIJQEqN"
      },
      "source": [
        "## Files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the dataset, annotations and extra features. If you don't want to use extra features don't run the last cell in this chapter."
      ],
      "metadata": {
        "id": "t-OJ3up6mCmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqAx6w_4SYah"
      },
      "outputs": [],
      "source": [
        "with open(base_path + 'easg.json', 'r') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSYCKd9pKfkr",
        "outputId": "472d82ea-e855-486a-c97b-76419dcd6559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "483\n",
            "227\n",
            "16\n"
          ]
        }
      ],
      "source": [
        "def create_word_index_dict(file_path):\n",
        "    word_index_dict = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        for index, line in enumerate(file, start=1):\n",
        "            word = line.strip()\n",
        "            word_index_dict[word] = index\n",
        "    return word_index_dict\n",
        "\n",
        "objects_path = base_path + 'objects.txt'\n",
        "verbs_path = base_path + 'verbs.txt'\n",
        "relationships_path = base_path + 'relationships.txt'\n",
        "\n",
        "o_dict = create_word_index_dict(objects_path)\n",
        "v_dict = create_word_index_dict(verbs_path)\n",
        "r_dict = create_word_index_dict(relationships_path)\n",
        "\n",
        "print(len(o_dict))\n",
        "print(len(v_dict))\n",
        "print(len(r_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16FN_rwviKbG"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "verb_feats = torch.load(base_path + 'verb_features.pt')\n",
        "with open(base_path + 'roi_feats_val.pkl', 'rb') as f:\n",
        "  roi_val_feats = pickle.load(f)\n",
        "with open(base_path + 'roi_feats_train.pkl', 'rb') as f:\n",
        "  roi_train_feats = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4FE-ptpP9nN"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJlF-NQFdXk"
      },
      "outputs": [],
      "source": [
        "def rindex(lst, item):\n",
        "    try:\n",
        "        return len(lst) - 1 - lst[::-1].index(item)\n",
        "    except ValueError:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01DMLrQR-dOd"
      },
      "outputs": [],
      "source": [
        "def calc_possible_seq(seq_len, data_obj_list):\n",
        "  non_valid = 0\n",
        "  valid = 0\n",
        "  tot_seq = 0\n",
        "  for data in data_obj_list:\n",
        "    if len(data.extra_features[\"graph_uids\"]) <= seq_len:\n",
        "      non_valid = non_valid + 1\n",
        "    if len(data.extra_features[\"graph_uids\"]) > seq_len:\n",
        "      valid = valid + 1\n",
        "      tot_seq = tot_seq + (len(data.extra_features[\"graph_uids\"]) - seq_len)\n",
        "  return non_valid, valid, tot_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrAnvig7qdPq"
      },
      "outputs": [],
      "source": [
        "def create_mask(original, id_list):\n",
        "    if isinstance(original, list):\n",
        "        return torch.tensor([x in id_list for x in original], dtype=torch.bool)\n",
        "    elif isinstance(original, torch.Tensor):\n",
        "        return torch.tensor([x.item() in id_list for x in original], dtype=torch.bool)\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "def init_model(model):\n",
        "    for name, param in model.named_parameters():\n",
        "          if 'weight' in name:\n",
        "            if 'lin' in name or 'linear' in name:\n",
        "              init.kaiming_normal_(param, mode='fan_in', nonlinearity='relu')"
      ],
      "metadata": {
        "id": "S5jfkjkxLEZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLuwzrFMQgSQ"
      },
      "source": [
        "## Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function creates a first representation of the dataset, suitable to be converted in an HeteroData object."
      ],
      "metadata": {
        "id": "7LqmchDEud0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6loZJIT-mflO"
      },
      "outputs": [],
      "source": [
        "def extract_from_sequence(seq, include_extracted_features = False):\n",
        "  num_obj_features = 13\n",
        "  num_extra_features = 1\n",
        "  num_verb_features = 2305\n",
        "  # One per sequence\n",
        "  objects = [] #semplice contatore\n",
        "  seen_objects = []\n",
        "  CW_to_seen = [[], []]\n",
        "  verbs = [] #funge da verb feature\n",
        "  v_to_o_edges = [[], []] # usa index in object feature pper identificare gli oggetti\n",
        "  v_to_o_attr = [] # usa index dell'edge per indetificare edge\n",
        "  o_to_o_edges = [[],[]]\n",
        "  o_to_o_attr = []\n",
        "  time_edges = [[],[]] # usa index oggetti, non ha features\n",
        "  dobj_edge = [[],[]]\n",
        "  previous_graph_objects = {} # struttura di controllo\n",
        "  current_graph_objects = {} # struttura di controllo\n",
        "  current_seen_objects = {}\n",
        "  objects_features = torch.empty(0, num_obj_features, dtype=torch.float) # unico index degli oggetti nella struttura finale\n",
        "  seen_objects_features = torch.empty(0, num_obj_features, dtype=torch.float)\n",
        "  graph_index_list = []\n",
        "  verb_uids = []\n",
        "  seen_obj_uid = []\n",
        "  CW_uids = []\n",
        "  extra_features = {\"split\": seq[\"split\"], \"video_uid\": seq[\"video_uid\"], \"shape\": [seq[\"W\"], seq[\"H\"]], \"graph_uids\": []}\n",
        "\n",
        "  for g in seq['graphs']:\n",
        "    extra_features['graph_uids'].append(g['graph_uid'])\n",
        "    current_verb = \"\" # reset\n",
        "    o_count = 0\n",
        "    duplicate_index = -1\n",
        "    CW_uids.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "    # Triplets\n",
        "    for ent1, rel, ent2 in g['triplets']:\n",
        "      if rel != 'verb':\n",
        "        if ent1 in v_dict.keys(): # Verb su Object\n",
        "          if current_verb == \"\": #prima tripla\n",
        "            current_verb = v_dict[ent1]\n",
        "            verbs.append(current_verb) #verbo è scelto per questo grafo\n",
        "            verb_uids.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "          elif current_verb != v_dict[ent1]:\n",
        "            print(f\"Error a: two verbs present {ent1} {ent2} {current_verb}\")\n",
        "            print(g['graph_uid'])\n",
        "            sys.exit()\n",
        "          if rel == 'dobj':\n",
        "            dobj_edge[0].append(len(verbs) - 1)\n",
        "            if o_dict[ent2] not in current_graph_objects.keys():\n",
        "              o_count = o_count + 1\n",
        "              current_graph_objects[o_dict[ent2]] = len(objects)\n",
        "              objects.append(o_dict[ent2])\n",
        "            dobj_edge[1].append(rindex(objects,o_dict[ent2]))\n",
        "          else:\n",
        "            v_to_o_edges[0].append(len(verbs) - 1) #ci si riferisce al verbo tramite l'indice (è ultimo)\n",
        "\n",
        "            if \":1\" in ent2: # Gestire caso in cui :1 viene prima del'oggetto originale\n",
        "              ent2 = ent2[:len(ent2)- 2]\n",
        "              o_count = o_count + 1\n",
        "              duplicate_index = len(objects)\n",
        "              objects.append(o_dict[ent2])\n",
        "              current_graph_objects[o_dict[ent2] + len(o_dict.keys())] = rindex(objects, o_dict[ent2]) #usato solo in costruzione grafo, nel tensore oggetto avrà indice corretto\n",
        "\n",
        "            elif o_dict[ent2] not in current_graph_objects.keys():\n",
        "              o_count = o_count + 1\n",
        "              current_graph_objects[o_dict[ent2]] = len(objects)\n",
        "              objects.append(o_dict[ent2])\n",
        "\n",
        "\n",
        "            v_to_o_edges[1].append(rindex(objects,o_dict[ent2])) # Si usa l'indice dell'oggetto presente tra gli ultimi aggiunti\n",
        "            v_to_o_attr.append(r_dict[rel])\n",
        "\n",
        "        else:\n",
        "          # controllare se oggetto è gia presente nei current objects,\n",
        "          # in caso contrario si aggiunge.\n",
        "          # Non dovrebbe esserci più oggetti dello stesso tipo nel grafo in cui si connettono tra loro oggetti\n",
        "            if o_dict[ent1] not in current_graph_objects.keys():\n",
        "                o_count = o_count + 1\n",
        "                current_graph_objects[o_dict[ent1]] = len(objects)\n",
        "                objects.append(o_dict[ent1])\n",
        "            o_to_o_edges[0].append(current_graph_objects[o_dict[ent1]])\n",
        "            if o_dict[ent2] not in current_graph_objects.keys():\n",
        "                o_count = o_count + 1\n",
        "                current_graph_objects[o_dict[ent2]] = len(objects)\n",
        "                objects.append(o_dict[ent2])\n",
        "            o_to_o_edges[1].append(current_graph_objects[o_dict[ent2]])\n",
        "            o_to_o_attr.append(r_dict[rel])\n",
        "      else:\n",
        "        if current_verb == \"\":\n",
        "          current_verb = v_dict[ent2]\n",
        "          verbs.append(current_verb)\n",
        "          verb_uids.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "        elif current_verb != v_dict[ent2]:\n",
        "          print(f\"Error b: two verbs present {ent1} {ent2} {current_verb}\")\n",
        "          print(g['graph_uid'])\n",
        "          sys.exit()\n",
        "    # Features\n",
        "    for j in range(0, o_count):\n",
        "      index = len(objects_features)\n",
        "      objects_features = torch.cat((objects_features, torch.zeros(num_obj_features).unsqueeze(0)), dim=0)\n",
        "      objects_features[index, 0] = objects[j]\n",
        "    i = 1\n",
        "    groundings = g['groundings']\n",
        "    for frame in groundings:\n",
        "        for key in groundings[frame].keys():\n",
        "            values = list(groundings[frame][key].values())\n",
        "            values = [float(v) for v in values]\n",
        "            values = torch.tensor(values, dtype = torch.float)\n",
        "            index = -1\n",
        "            if \":1\" in key and duplicate_index != -1:\n",
        "                index = duplicate_index\n",
        "                key = key[:len(key)-2]\n",
        "            else:\n",
        "                if key in o_dict.keys() and o_dict[key] in current_graph_objects.keys():\n",
        "                    index = current_graph_objects[o_dict[key]]\n",
        "            if index != -1:\n",
        "                for j in range(0, 4):\n",
        "                    objects_features[index, j + i] = values[j]\n",
        "            else:\n",
        "              if o_dict[key] not in current_seen_objects.keys():\n",
        "                CW_to_seen[0].append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "                CW_to_seen[1].append(len(seen_objects_features))\n",
        "                current_seen_objects[o_dict[key]] = len(seen_objects_features)\n",
        "                index = current_seen_objects[o_dict[key]]\n",
        "                seen_objects_features = torch.cat((seen_objects_features, torch.zeros(num_obj_features).unsqueeze(0)), dim=0)\n",
        "                seen_objects_features[-1][0] = o_dict[key]\n",
        "                seen_obj_uid.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "              else:\n",
        "                index = current_seen_objects[o_dict[key]]\n",
        "              for j in range(0, 4):\n",
        "                    seen_objects_features[index, j + i] = values[j]\n",
        "        i = i + 4\n",
        "    #ID assignment\n",
        "    for j in range(0, o_count):\n",
        "       graph_index_list.append(extra_features['graph_uids'].index(g['graph_uid']))\n",
        "    #Time edges\n",
        "    if previous_graph_objects != {}:\n",
        "        for key in previous_graph_objects.keys():\n",
        "            if key in current_graph_objects.keys():\n",
        "                time_edges[0].append(previous_graph_objects[key])\n",
        "                time_edges[1].append(current_graph_objects[key])\n",
        "    previous_graph_objects = current_graph_objects\n",
        "    current_graph_objects = {}\n",
        "    current_seen_objects = {}\n",
        "  #v to v\n",
        "  v_to_v = [[], []]\n",
        "  for i in range(len(verbs) - 1):\n",
        "    v_to_v[0].append(i)\n",
        "    v_to_v[1].append(i + 1)\n",
        "  # Verbs features\n",
        "  verbs_features = torch.tensor(verbs, dtype=torch.float)\n",
        "  if include_extracted_features:\n",
        "    verbs_features = torch.empty(0, num_verb_features, dtype=torch.float)\n",
        "    for i in range(len(verbs)):\n",
        "      t = torch.cat((torch.tensor([verbs[i]]), verb_feats[extra_features['graph_uids'][verb_uids[i]]]))\n",
        "      verbs_features = torch.cat((verbs_features, t.unsqueeze(0)), dim=0)\n",
        "  # Seen objects\n",
        "  s_index = len(objects_features)\n",
        "  objects_features = torch.cat((objects_features, seen_objects_features), dim=0)\n",
        "  obj_graph_uids = torch.tensor(graph_index_list + seen_obj_uid, dtype=torch.int64)\n",
        "  for i in range(len(CW_to_seen[0])):\n",
        "    CW_to_seen[1][i] = CW_to_seen[1][i] + s_index\n",
        "\n",
        "  dictionary = {}\n",
        "  if include_extracted_features:\n",
        "    dictionary['verbs_features'] = verbs_features\n",
        "  else:\n",
        "    dictionary['verbs_features'] = verbs_features.unsqueeze(1)\n",
        "  dictionary['obj_features'] = objects_features\n",
        "  # dictionary['seen_obj_features'] = seen_objects_features\n",
        "  dictionary['v_to_o'] = torch.tensor(v_to_o_edges, dtype=torch.int64)\n",
        "  dictionary['v_to_o_attr'] = torch.tensor(v_to_o_attr, dtype=torch.float)\n",
        "  dictionary['dobj_edge'] = torch.tensor(dobj_edge, dtype=torch.int64)\n",
        "  dictionary['o_to_o'] = torch.tensor(o_to_o_edges, dtype=torch.int64)\n",
        "  dictionary['o_to_o_attr'] = torch.tensor(o_to_o_attr, dtype=torch.float)\n",
        "  dictionary['time_edges'] = torch.tensor(time_edges, dtype=torch.int64)\n",
        "  dictionary['v_to_v'] = torch.tensor(v_to_v, dtype=torch.int64)\n",
        "  dictionary['CW_to_seen'] = torch.tensor(CW_to_seen, dtype=torch.int64)\n",
        "  dictionary['obj_graph_uids'] = obj_graph_uids\n",
        "  dictionary['verb_graph_uids'] = torch.tensor(verb_uids, dtype=torch.int64)\n",
        "  # dictionary['seen_obj_graph_uids'] = torch.tensor(seen_obj_uid, dtype=torch.int64)\n",
        "  dictionary['CW_graph_uids'] = torch.tensor(CW_uids, dtype=torch.int64)\n",
        "  dictionary['extra'] = extra_features\n",
        "\n",
        "  return dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through this method the actual HeteroData object is created. It represents the original sequence present in the dataset."
      ],
      "metadata": {
        "id": "Ik3LTkvNup3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsGVE-07pest"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "def createHeteroData(source):\n",
        "  data = HeteroData()\n",
        "\n",
        "  data['verb'].x = source['verbs_features']\n",
        "  data['object'].x = source['obj_features']\n",
        "  # data['seen_object'].x = source['seen_obj_features']\n",
        "  data['CW'].num_nodes = len(source['verbs_features'])\n",
        "\n",
        "  data['verb', 'rel', 'object'].edge_index = source['v_to_o']\n",
        "  data['verb', 'rel', 'object'].edge_attr = source['v_to_o_attr']\n",
        "  data['verb', 'dobj', 'object'].edge_index = source['dobj_edge']\n",
        "  data['object', 'rel', 'object'].edge_index = source['o_to_o']\n",
        "  data['object', 'rel', 'object'].edge_attr = source['o_to_o_attr']\n",
        "  data['object', 'time', 'object'].edge_index = source['time_edges']\n",
        "  data['verb', 'next', 'verb'].edge_index = source['v_to_v']\n",
        "  data['CW', 'sees', 'object'].edge_index = source['CW_to_seen']\n",
        "\n",
        "  data['object'].extra_features = source['obj_graph_uids']\n",
        "  data['verb'].extra_features = source['verb_graph_uids']\n",
        "  # data['seen_object'].extra_features = source['seen_obj_graph_uids']\n",
        "  data['CW'].extra_features = source['CW_graph_uids']\n",
        "\n",
        "  data.extra_features = source['extra']\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QDQvbIbcJVt"
      },
      "outputs": [],
      "source": [
        "def from_json_to_heteroData_list(data):\n",
        "  var = 0\n",
        "  data_list = []\n",
        "  for d in data.keys():\n",
        "    graph_dict = extract_from_sequence(data[d], include_extracted_features)\n",
        "    if var == 0:\n",
        "      var += 1\n",
        "    h_data = createHeteroData(graph_dict)\n",
        "    if var == 1:\n",
        "      var = 2\n",
        "    data_list.append(h_data)\n",
        "  return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is used to create the subsequences on which the models are going to be trained. The length is decided bby the length of the span argument and two parameteres are provided to allow specific augumentations."
      ],
      "metadata": {
        "id": "E-tmjNMGu4es"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzuIM_I0np9Z"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def extract_subgraph(original, span, include_future_object = False, word2vec = False):\n",
        "  label = {'verb': -1, 'object': -1}\n",
        "  target_index = span[-1] + 1\n",
        "  verb_idx = torch.nonzero(create_mask(original['verb'].extra_features, [target_index]))\n",
        "  label['verb'] = int(original['verb'].x[int(verb_idx)][0])\n",
        "  for index in range(len(original['verb', 'dobj', 'object'].edge_index[0])):\n",
        "    if original['verb', 'dobj', 'object'].edge_index[0][index] == verb_idx:\n",
        "      label['object'] = int(original['object'].x[original['verb', 'dobj', 'object'].edge_index[1][index]][0])\n",
        "      break\n",
        "  \"\"\"\n",
        "  label_objects = torch.nonzero(create_mask(original['object'].extra_features, [target_index]))\n",
        "  for index in range(len(original['verb', 'rel', 'object'].edge_index[0])):\n",
        "    if original['verb', 'rel', 'object'].edge_index[1][index] in label_objects and original['verb', 'rel', 'object'].edge_attr[index] == r_dict['dobj']:\n",
        "      label['object'] = int(original['object'].x[original['verb', 'rel', 'object'].edge_index[1][index]][0])\n",
        "      break\"\"\"\n",
        "  if(label['object'] == -1):\n",
        "    print(\"NO LABEL\")\n",
        "    sys.exit()\n",
        "  masks = {}\n",
        "  for node_type in original.node_types:\n",
        "    masks[node_type] = create_mask(original[node_type].extra_features, span)\n",
        "  subgraph_data = original.subgraph(masks)\n",
        "  subgraph_data.extra_features = original.extra_features.copy()\n",
        "  subgraph_data.extra_features['graph_uids'] = copy.deepcopy(original.extra_features['graph_uids'][span[0]:span[0]+len(span)])\n",
        "  subgraph_data.extra_features['label_uid'] = original.extra_features['graph_uids'][target_index]\n",
        "  inv_mask = ~masks['object']\n",
        "\n",
        "  if not include_future_object:\n",
        "    inv_mask[int(torch.nonzero(masks['object'])[0]):] = False\n",
        "  other_obj = original['object'].x[inv_mask]\n",
        "  other_obj_extra = original['object'].extra_features[inv_mask]\n",
        "  other_edges = [[],[]]\n",
        "  s_index = len(subgraph_data['object'].x)\n",
        "  subgraph_data['object'].x = torch.cat((subgraph_data['object'].x, other_obj), dim=0)\n",
        "  subgraph_data['object'].extra_features = torch.cat((subgraph_data['object'].extra_features, other_obj_extra), dim=0)\n",
        "  for i in range(len(other_obj)):\n",
        "    for j in range(len(span)):\n",
        "      other_edges[0].append(i + s_index)\n",
        "      other_edges[1].append(j)\n",
        "  subgraph_data['object', 'has_seen', 'CW'].edge_index = torch.tensor(other_edges, dtype=torch.int64)\n",
        "  other_edges.reverse()\n",
        "  subgraph_data['CW', 'has_seen', 'object'].edge_index = torch.tensor(other_edges, dtype=torch.int64)\n",
        "  subgraph_data.y_verb = torch.tensor([label['verb']], dtype=torch.int64)\n",
        "  subgraph_data.y_obj = torch.tensor([label['object']], dtype=torch.int64)\n",
        "  subgraph_data['CW'].x = torch.zeros(subgraph_data['CW'].num_nodes, 1, dtype=torch.float)\n",
        "\n",
        "  if word2vec:\n",
        "    subgraph_data = create_embeddings(subgraph_data)\n",
        "  return remove_empty(subgraph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmOmpVPJE_2i"
      },
      "outputs": [],
      "source": [
        "def remove_empty(data):\n",
        "  to_remove = []\n",
        "  for node_type in data.node_types:\n",
        "    if 'x' in data[node_type].keys() and data[node_type].x.size(0) == 0:\n",
        "      to_remove.append(node_type)\n",
        "\n",
        "  for edge_typpe in data.edge_types:\n",
        "    if 'edge_index' in data[node_type].keys() and data[node_type].edge_index.size(0) == 0:\n",
        "      to_remove.append(node_type)\n",
        "\n",
        "  for node_type in to_remove:\n",
        "    del data[node_type]\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two methods manage the creation of Word2Vec embeddings. Some annotations were not compatible with the chosen Word2Vec model so they had to be translated."
      ],
      "metadata": {
        "id": "43SWmJ5VvVAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfKHyxAWndgS"
      },
      "outputs": [],
      "source": [
        "rev_o_dict = {v: k for k, v in o_dict.items()}\n",
        "rev_v_dict = {v: k for k, v in v_dict.items()}\n",
        "rev_r_dict = {v: k for k, v in r_dict.items()}\n",
        "rev_r_dict[r_dict['dobj']] ='direct object'\n",
        "rev_r_dict[r_dict['to']] ='To'\n",
        "rev_v_dict[v_dict['unhang']] = 'not hang'\n",
        "\n",
        "def create_embedding(label):\n",
        "  embeddings= []\n",
        "  for word in label.split():\n",
        "    embeddings.append(torch.tensor(w2v[word]))\n",
        "  final_embedding = []\n",
        "  for embd in embeddings:\n",
        "    if final_embedding == []:\n",
        "      final_embedding = torch.tensor(embd, dtype=torch.float)\n",
        "    else:\n",
        "      final_embedding = torch.mean(torch.stack([final_embedding, embd], dim=0), dim=0)\n",
        "  return final_embedding\n",
        "\n",
        "def create_embeddings(hData):\n",
        "  for node_type, y in hData.node_items():\n",
        "    u_dict = rev_o_dict\n",
        "    if 'x' in y.keys() and node_type != 'CW':\n",
        "      if hData[node_type].x.numel() != 0:\n",
        "        new_x = torch.empty(0, len(hData[node_type].x[0]) + 299, dtype=torch.float)\n",
        "        if node_type == 'verb':\n",
        "          u_dict = rev_v_dict\n",
        "        for i in range(0, len(hData[node_type].x)):\n",
        "          obj_embedding = create_embedding(u_dict[int(hData[node_type].x[i][0])])\n",
        "          existing_features = hData[node_type].x[i][1:]\n",
        "          obj = torch.cat((obj_embedding, existing_features), dim=0)\n",
        "          new_x = torch.cat((new_x, obj.unsqueeze(0)), dim=0)\n",
        "        hData[node_type].x = new_x\n",
        "  for edge_type, y in hData.edge_items():\n",
        "    if 'edge_attr' in y.keys() and len(hData[edge_type].edge_attr) >0:\n",
        "      original_t = hData[edge_type].edge_attr\n",
        "      embeddings = []\n",
        "      for i in range(len(hData[edge_type].edge_attr)):\n",
        "        embeddings.append(create_embedding(rev_r_dict[int(hData[edge_type].edge_attr[i])]))\n",
        "      hData[edge_type].edge_attr = torch.stack(embeddings)\n",
        "  return hData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ZbFYGTrFyf"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Dataset\n",
        "\n",
        "class HeteroDataset(Dataset):\n",
        "    def __init__(self, data_list):\n",
        "        super().__init__()\n",
        "        self.data_list = data_list\n",
        "        self._indices = None\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data_list[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXn-qs6ezKPC"
      },
      "outputs": [],
      "source": [
        "def create_samples(data_obj_list, seq_length):\n",
        "  data_list = []\n",
        "  id_lists = []\n",
        "  for hData in data_obj_list:\n",
        "    if len(hData.extra_features[\"graph_uids\"]) > seq_length:\n",
        "      id_lists.append(hData.extra_features[\"video_uid\"])\n",
        "      for i in range(len(hData.extra_features[\"graph_uids\"]) - seq_length):\n",
        "        data_list.append(extract_subgraph(hData, range(i, i + seq_length), include_future_object, word2vec=word2vec))\n",
        "  return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlLlU4paRGl6"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyperparameters manage the parsing of the dataset."
      ],
      "metadata": {
        "id": "pmuZY9YevnGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiK7EYGDQ6A1"
      },
      "outputs": [],
      "source": [
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyperparameters influence the training process."
      ],
      "metadata": {
        "id": "MtaHpErsvr0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i00bNJooA_-g"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBm_hrDZ2x0F",
        "outputId": "95ee8d42-a0c6-493c-ae16-9d06a563ea24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The active device is cuda\n"
          ]
        }
      ],
      "source": [
        "device ='cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "print(f\"The active device is {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbCVKPcnW8-7"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each of the following chapter a different type of model is described. The classifier is included only to provide an example of the general architecture."
      ],
      "metadata": {
        "id": "AQrc8n_mv1Ia"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Q_ienGCrDr"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE5F-5F6gem9"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Linear, to_hetero\n",
        "from torch.nn import Softmax, Parameter\n",
        "\n",
        "class GNNWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        model = GNN(hidden_channels, hidden_channels)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "        self.linear_verb = Linear(hidden_channels, out_channels[0])\n",
        "        self.linear_obj = Linear(hidden_channels, out_channels[1])\n",
        "\n",
        "        self.alpha = Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "        self.beta = Parameter(torch.tensor(1.0), requires_grad=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gnn(x, edge_index)\n",
        "\n",
        "        x_verb = self.linear_verb(x['verb'])\n",
        "        x_obj = self.linear_obj(x['object'])\n",
        "\n",
        "        return x_verb, x_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAGEConv"
      ],
      "metadata": {
        "id": "x8szjkN8LBN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import Linear, to_hetero, SAGEConv\n",
        "from torch.nn import Softmax, Parameter\n",
        "import torch.nn.init as init\n",
        "\n",
        "class SAGEConvGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv3 = SAGEConv((-1, -1), out_channels)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class SAGEWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        model = SAGEConvGNN(hidden_channels, hidden_channels)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "\n",
        "        self.lin1 = Linear(-1, hidden_channels)\n",
        "        self.lin2 = Linear(-1, hidden_channels)\n",
        "        self.lin3 = Linear(-1, hidden_channels)\n",
        "\n",
        "        self.linear_verb = Linear(-1, out_channels[0])\n",
        "        self.linear_obj = Linear(-1, out_channels[1])\n",
        "\n",
        "        self.theta = Parameter(torch.randn(1))\n",
        "        self.register_buffer('alpha', torch.zeros(1))\n",
        "        self.register_buffer('beta', torch.zeros(1))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        self.beta = torch.sigmoid(self.theta)\n",
        "        self.alpha = 1 - self.beta\n",
        "\n",
        "        x = self.gnn(x, edge_index)\n",
        "\n",
        "        x_verb = self.lin1(x['verb']).relu()\n",
        "        x_verb = self.lin2(x_verb).relu()\n",
        "        x_verb = self.lin3(x_verb).relu()\n",
        "        x_verb = self.linear_verb(x_verb)\n",
        "\n",
        "        x_obj = self.lin1(x['object']).relu()\n",
        "        x_obj = self.lin2(x_obj).relu()\n",
        "        x_obj = self.lin3(x_obj).relu()\n",
        "        x_obj = self.linear_obj(x_obj)\n",
        "\n",
        "        return x_verb, x_obj"
      ],
      "metadata": {
        "id": "WwW_xCtmLApZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwA14VH1CzOY"
      },
      "source": [
        "### GAT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZh9oML0Ff6Y"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAT\n",
        "from torch_geometric.nn import Linear, to_hetero\n",
        "from torch.nn import Softmax, Parameter\n",
        "\n",
        "\n",
        "class GATWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        model = GAT((-1, -1), hidden_channels, out_channels=hidden_channels, num_layers=num_layers, add_self_loops=False)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
        "        self.linear_verb = Linear(hidden_channels, out_channels[0])\n",
        "        self.linear_obj = Linear(hidden_channels, out_channels[1])\n",
        "\n",
        "        self.theta = Parameter(torch.randn(1))\n",
        "        self.register_buffer('alpha', torch.zeros(1))\n",
        "        self.register_buffer('beta', torch.zeros(1))\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        self.beta = torch.sigmoid(self.theta)\n",
        "        self.alpha = 1 - self.beta\n",
        "\n",
        "        if edge_attr == None:\n",
        "          x = self.gnn(x, edge_index)\n",
        "        else:\n",
        "          x = self.gnn(x, edge_index, edge_attr)\n",
        "\n",
        "        x_verb = self.lin1(x['verb']).relu()\n",
        "        x_verb = self.lin2(x_verb).relu()\n",
        "        x_verb = self.lin3(x_verb).relu()\n",
        "        x_verb = self.linear_verb(x_verb)\n",
        "\n",
        "        x_obj = self.lin1(x['object']).relu()\n",
        "        x_obj = self.lin2(x_obj).relu()\n",
        "        x_obj = self.lin3(x_obj).relu()\n",
        "        x_obj = self.linear_obj(x_obj)\n",
        "\n",
        "        return x_verb, x_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87NNXoZqC3RK"
      },
      "source": [
        "### Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpYetD3wcL4S"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import Linear, to_hetero, TransformerConv\n",
        "from torch.nn import Softmax, Parameter\n",
        "import torch.nn.init as init\n",
        "\n",
        "class TransformerConvGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = TransformerConv((-1, -1), hidden_channels, heads=num_layers)\n",
        "        self.conv2 = TransformerConv((-1, -1), hidden_channels, heads=num_layers)\n",
        "        self.conv3 = TransformerConv((-1, -1), out_channels, heads=num_layers)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class TransformerWithClassifier(torch.nn.Module):\n",
        "    def __init__(self, metadata, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        model = TransformerConvGNN(hidden_channels, hidden_channels)\n",
        "        self.gnn = to_hetero(model, metadata, aggr='sum')\n",
        "\n",
        "        self.lin1 = Linear(-1, hidden_channels)\n",
        "        self.lin2 = Linear(-1, hidden_channels)\n",
        "        self.lin3 = Linear(-1, hidden_channels)\n",
        "\n",
        "        self.linear_verb = Linear(-1, out_channels[0])\n",
        "        self.linear_obj = Linear(-1, out_channels[1])\n",
        "\n",
        "        self.theta = Parameter(torch.randn(1))\n",
        "        self.register_buffer('alpha', torch.zeros(1))\n",
        "        self.register_buffer('beta', torch.zeros(1))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        self.beta = torch.sigmoid(self.theta)\n",
        "        self.alpha = 1 - self.beta\n",
        "\n",
        "        x = self.gnn(x, edge_index)\n",
        "\n",
        "        x_verb = self.lin1(x['verb']).relu()\n",
        "        x_verb = self.lin2(x_verb).relu()\n",
        "        x_verb = self.lin3(x_verb).relu()\n",
        "        x_verb = self.linear_verb(x_verb)\n",
        "\n",
        "        x_obj = self.lin1(x['object']).relu()\n",
        "        x_obj = self.lin2(x_obj).relu()\n",
        "        x_obj = self.lin3(x_obj).relu()\n",
        "        x_obj = self.linear_obj(x_obj)\n",
        "\n",
        "        return x_verb, x_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48c77gbx0_D6"
      },
      "source": [
        "## Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two following functions manage the training and testing of the models and their outputs."
      ],
      "metadata": {
        "id": "WvMktGPmwIum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9_W28MsjB4o"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def process_logits(verb_logits, object_logits, batch_dict):\n",
        "  batches = torch.unique(batch_dict['verb'])\n",
        "  v_temp_list = []\n",
        "  o_temp_list = []\n",
        "  for b in batches:\n",
        "\n",
        "    v_indices = torch.nonzero(batch_dict['verb'] == b, as_tuple=False).squeeze(1)\n",
        "    v_temp = torch.mean(verb_logits[v_indices], dim=0, keepdim=True)\n",
        "    v_temp_list.append(v_temp)\n",
        "\n",
        "    o_indices = torch.nonzero(batch_dict['object'] == b, as_tuple=False).squeeze(1)\n",
        "    o_temp = torch.mean(object_logits[o_indices], dim=0, keepdim=True)\n",
        "    o_temp_list.append(o_temp)\n",
        "\n",
        "  v_temp_matrix = torch.cat(v_temp_list, dim=0)\n",
        "  v_temp_matrix = F.softmax(v_temp_matrix, dim=1)\n",
        "\n",
        "  o_temp_matrix = torch.cat(o_temp_list, dim=0)\n",
        "  o_temp_matrix = F.softmax(o_temp_matrix, dim=1)\n",
        "\n",
        "  return v_temp_matrix, o_temp_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML2xHMvC4xXI"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "def train(model, train_loader, optimizer, scheduler, device = 'cpu', epochs = 10, val_loader = None, extra_features = False, trained_loss = False, loss_weights=(1,1)):\n",
        "    model = model.to(device)\n",
        "\n",
        "    history = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_verb_correct_1 = 0\n",
        "        total_object_correct_1 = 0\n",
        "        total_verb_correct_5 = 0\n",
        "        total_object_correct_5 = 0\n",
        "        total_action_correct_1 = 0\n",
        "        total_action_correct_5 = 0\n",
        "        total_verb_loss = 0\n",
        "        total_object_loss = 0\n",
        "\n",
        "        for data in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if extra_features:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict)\n",
        "            else:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict)\n",
        "            n_verb_logits, n_object_logits = process_logits(verb_logits, object_logits, data.batch_dict)\n",
        "            loss1 = F.cross_entropy(n_verb_logits, data.y_verb)\n",
        "            loss2 = F.cross_entropy(n_object_logits, data.y_obj)\n",
        "            if trained_loss:\n",
        "              loss = (10 * model.alpha * loss1) + (model.beta * loss2 * 10)\n",
        "            else:\n",
        "              loss = loss_weights[0] * loss1 + loss_weights[1] * loss2\n",
        "            total_verb_loss += loss1.item()\n",
        "            total_object_loss += loss2.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, verb_logits_topk = torch.topk(n_verb_logits, 5, largest=True, sorted=True)\n",
        "            _, object_logits_topk = torch.topk(n_object_logits, 5, largest=True, sorted=True)\n",
        "\n",
        "            total_verb_correct_1 += (verb_logits_topk[:, 0] == data.y_verb).sum().item()\n",
        "            total_object_correct_1 += (object_logits_topk[:, 0] == data.y_obj).sum().item()\n",
        "            total_action_correct_1 += ((verb_logits_topk[:, 0] == data.y_verb) & (object_logits_topk[:, 0] == data.y_obj)).sum().item()\n",
        "\n",
        "            total_verb_correct_5 += sum([data.y_verb[i] in verb_logits_topk[i] for i in range(data.y_verb.size(0))])\n",
        "            total_object_correct_5 += sum([data.y_obj[i] in object_logits_topk[i] for i in range(data.y_obj.size(0))])\n",
        "            total_action_correct_5 += sum([(data.y_verb[i] in verb_logits_topk[i]) & (data.y_obj[i] in object_logits_topk[i]) for i in range(data.y_obj.size(0))])\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataset)\n",
        "        train_metrics = {\n",
        "            'lr': optimizer.param_groups[0]['lr'],\n",
        "            'train_loss': avg_loss,\n",
        "            'train_verb_loss': total_verb_loss,\n",
        "            'train_obj_loss': total_object_loss,\n",
        "            'train_v_acc_1': total_verb_correct_1 / len(train_dataset),\n",
        "            'train_o_acc_1': total_object_correct_1 / len(train_dataset),\n",
        "            'train_a_acc_1': total_action_correct_1 / len(train_dataset),\n",
        "            'train_v_acc_5': total_verb_correct_5 / len(train_dataset),\n",
        "            'train_o_acc_5': total_object_correct_5 / len(train_dataset),\n",
        "            'train_a_acc_5': total_action_correct_5 / len(train_dataset)\n",
        "        }\n",
        "        if trained_loss:\n",
        "          train_metrics['loss alpha'] = model.alpha.item()\n",
        "          train_metrics['loss beta'] = model.beta.item()\n",
        "        if val_loader is not None:\n",
        "          val_metrics = validate(model, val_loader, device, extra_features, trained_loss, loss_weights)\n",
        "\n",
        "        history.append({**train_metrics, **val_metrics})\n",
        "        wandb.log({**train_metrics, **val_metrics})\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {avg_loss:.4f}, \"\n",
        "              f\"Verb Acc@1: {train_metrics['train_v_acc_1']*100:.2f}%, \"\n",
        "              f\"Object Acc@1: {train_metrics['train_o_acc_1']*100:.2f}%, \"\n",
        "              f\"Action Acc@1: {train_metrics['train_a_acc_1']*100:.2f}%, \"\n",
        "              f\"Verb Acc@5: {train_metrics['train_v_acc_5']*100:.2f}%, \"\n",
        "              f\"Object Acc@5: {train_metrics['train_o_acc_5']*100:.2f}%, \"\n",
        "              f\"Action Acc@5: {train_metrics['train_a_acc_5']*100:.2f}%\")\n",
        "        if val_dataset is not None:\n",
        "          print(f\"Validation Loss: {val_metrics['val_loss']:.4f}, \"\n",
        "                f\"Verb Acc@1: {val_metrics['val_v_acc_1']*100:.2f}%, \"\n",
        "                f\"Object Acc@1: {val_metrics['val_o_acc_1']*100:.2f}%, \"\n",
        "                f\"Action Acc@1: {val_metrics['val_a_acc_1']*100:.2f}%, \"\n",
        "                f\"Verb Acc@5: {val_metrics['val_v_acc_5']*100:.2f}%, \"\n",
        "                f\"Object Acc@5: {val_metrics['val_o_acc_5']*100:.2f}%, \"\n",
        "                f\"Action Acc@5: {val_metrics['val_a_acc_5']*100:.2f}%\")\n",
        "          print(f\"Loss alpha: {model.alpha},\"\n",
        "                f\"Loss beta: {model.beta}\")\n",
        "    return history\n",
        "\n",
        "def validate(model, val_loader, device, extra_features=False, trained_loss = False, loss_weights=(1,1)):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_verb_correct_1 = 0\n",
        "    total_object_correct_1 = 0\n",
        "    total_verb_correct_5 = 0\n",
        "    total_object_correct_5 = 0\n",
        "    total_action_correct_1 = 0\n",
        "    total_action_correct_5 = 0\n",
        "    total_verb_loss = 0\n",
        "    total_object_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(val_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "\n",
        "            if extra_features:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict)\n",
        "            else:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict)\n",
        "            n_verb_logits, n_object_logits = process_logits(verb_logits, object_logits, data.batch_dict)\n",
        "            loss1 = F.cross_entropy(n_verb_logits, data.y_verb)\n",
        "            loss2 = F.cross_entropy(n_object_logits, data.y_obj)\n",
        "            if trained_loss:\n",
        "              loss = (10 * model.alpha * loss1) + (model.beta * loss2 * 10)\n",
        "            else:\n",
        "              loss = loss_weights[0] * loss1 + loss_weights[1] * loss2\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, verb_logits_topk = torch.topk(n_verb_logits, 5, largest=True, sorted=True)\n",
        "            _, object_logits_topk = torch.topk(n_object_logits, 5, largest=True, sorted=True)\n",
        "\n",
        "            total_verb_correct_1 += (verb_logits_topk[:, 0] == data.y_verb).sum().item()\n",
        "            total_object_correct_1 += (object_logits_topk[:, 0] == data.y_obj).sum().item()\n",
        "            total_action_correct_1 += ((verb_logits_topk[:, 0] == data.y_verb) & (object_logits_topk[:, 0] == data.y_obj)).sum().item()\n",
        "\n",
        "            total_verb_correct_5 += sum([data.y_verb[i] in verb_logits_topk[i] for i in range(data.y_verb.size(0))])\n",
        "            total_object_correct_5 += sum([data.y_obj[i] in object_logits_topk[i] for i in range(data.y_obj.size(0))])\n",
        "            total_action_correct_5 += sum([(data.y_verb[i] in verb_logits_topk[i]) & (data.y_obj[i] in object_logits_topk[i]) for i in range(data.y_obj.size(0))])\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataset)\n",
        "    val_metrics = {\n",
        "        'val_loss': avg_loss,\n",
        "        'val_verb_loss': total_verb_loss,\n",
        "        'val_obj_loss': total_object_loss,\n",
        "        'val_v_acc_1': total_verb_correct_1 / len(val_dataset),\n",
        "        'val_o_acc_1': total_object_correct_1 / len(val_dataset),\n",
        "        'val_a_acc_1': total_action_correct_1 / len(val_dataset),\n",
        "        'val_v_acc_5': total_verb_correct_5 / len(val_dataset),\n",
        "        'val_o_acc_5': total_object_correct_5 / len(val_dataset),\n",
        "        'val_a_acc_5': total_action_correct_5 / len(val_dataset)\n",
        "    }\n",
        "\n",
        "    return val_metrics\n",
        "\n",
        "def test(model, test_loader, device, extra_features=False, trained_loss = False, loss_weights=(1,1)):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_verb_correct_1 = 0\n",
        "    total_object_correct_1 = 0\n",
        "    total_verb_correct_5 = 0\n",
        "    total_object_correct_5 = 0\n",
        "    total_action_correct_1 = 0\n",
        "    total_action_correct_5 = 0\n",
        "    total_verb_loss = 0\n",
        "    total_object_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(val_loader, desc=\"Testing\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "\n",
        "            if extra_features:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict, data.edge_attr_dict)\n",
        "            else:\n",
        "              verb_logits, object_logits = model(data.x_dict, data.edge_index_dict)\n",
        "            n_verb_logits, n_object_logits = process_logits(verb_logits, object_logits, data.batch_dict)\n",
        "            loss1 = F.cross_entropy(n_verb_logits, data.y_verb)\n",
        "            loss2 = F.cross_entropy(n_object_logits, data.y_obj)\n",
        "            if trained_loss:\n",
        "              loss = (10 * model.alpha * loss1) + (model.beta * loss2 * 10)\n",
        "            else:\n",
        "              loss = loss_weights[0] * loss1 + loss_weights[1] * loss2\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, verb_logits_topk = torch.topk(n_verb_logits, 5, largest=True, sorted=True)\n",
        "            _, object_logits_topk = torch.topk(n_object_logits, 5, largest=True, sorted=True)\n",
        "\n",
        "            total_verb_correct_1 += (verb_logits_topk[:, 0] == data.y_verb).sum().item()\n",
        "            total_object_correct_1 += (object_logits_topk[:, 0] == data.y_obj).sum().item()\n",
        "            total_action_correct_1 += ((verb_logits_topk[:, 0] == data.y_verb) & (object_logits_topk[:, 0] == data.y_obj)).sum().item()\n",
        "\n",
        "            total_verb_correct_5 += sum([data.y_verb[i] in verb_logits_topk[i] for i in range(data.y_verb.size(0))])\n",
        "            total_object_correct_5 += sum([data.y_obj[i] in object_logits_topk[i] for i in range(data.y_obj.size(0))])\n",
        "            total_action_correct_5 += sum([(data.y_verb[i] in verb_logits_topk[i]) & (data.y_obj[i] in object_logits_topk[i]) for i in range(data.y_obj.size(0))])\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataset)\n",
        "    test_metrics = {\n",
        "        'test_loss': avg_loss,\n",
        "        'test_verb_loss': total_verb_loss,\n",
        "        'test_obj_loss': total_object_loss,\n",
        "        'test_v_acc_1': total_verb_correct_1 / len(val_dataset),\n",
        "        'test_o_acc_1': total_object_correct_1 / len(val_dataset),\n",
        "        'test_a_acc_1': total_action_correct_1 / len(val_dataset),\n",
        "        'test_v_acc_5': total_verb_correct_5 / len(val_dataset),\n",
        "        'test_o_acc_5': total_object_correct_5 / len(val_dataset),\n",
        "        'test_a_acc_5': total_action_correct_5 / len(val_dataset)\n",
        "    }\n",
        "    print(f\"Validation Loss: {test_metrics['test_loss']:.4f}, \"\n",
        "                f\"Verb Acc@1: {test_metrics['test_v_acc_1']*100:.2f}%, \"\n",
        "                f\"Object Acc@1: {test_metrics['test_o_acc_1']*100:.2f}%, \"\n",
        "                f\"Action Acc@1: {test_metrics['test_a_acc_1']*100:.2f}%, \"\n",
        "                f\"Verb Acc@5: {test_metrics['test_v_acc_5']*100:.2f}%, \"\n",
        "                f\"Object Acc@5: {test_metrics['test_o_acc_5']*100:.2f}%, \"\n",
        "                f\"Action Acc@5: {test_metrics['test_a_acc_5']*100:.2f}%\")\n",
        "    wandb.log({**test_metrics})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "wL8pRWJ2759J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every one of the following chapters contains a different set of hyperparameters to easily manage the execution of different experiments. Execute one at a time in between experimental runs."
      ],
      "metadata": {
        "id": "-o-soPt_wUbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline"
      ],
      "metadata": {
        "id": "itUQ2skn8WtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "HlQ9d5FS744u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No extra"
      ],
      "metadata": {
        "id": "9gd2dOqI8mcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= True\n",
        "include_extracted_features = False #<----------\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "AzXEvhM18pc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No Word2Vec"
      ],
      "metadata": {
        "id": "4aKd1XBt84g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = True\n",
        "word2vec= False #<----------\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "35h40cvV9Amy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No future object"
      ],
      "metadata": {
        "id": "72s8UxKj9FAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = True\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = False #<----------\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "Dha-PNYE9JRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No trained loss"
      ],
      "metadata": {
        "id": "Nh1fafIO-iMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "epochs = 20\n",
        "epoch_max = 10\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "num_layers = 4\n",
        "hidden_channels = 256\n",
        "extra_features = False\n",
        "trained_loss = False #<----------\n",
        "init_weights = False\n",
        "loss_weights = (1,2)\n",
        "\n",
        "seq_length = 20\n",
        "samples = 3000\n",
        "\n",
        "include_future_object = False\n",
        "word2vec= True\n",
        "include_extracted_features = True\n",
        "\n",
        "out_channels = (len(v_dict), len(o_dict))"
      ],
      "metadata": {
        "id": "LVbVr9Of-lr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WKVddwtRKZ6"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, is situated the running core of the notebook. The parsing, creation of dataset and dataloader, instatiation of the model and accompaning tools are all managed here. When the training starts, a free WandB keys is needed to log the results into an experiment tracker."
      ],
      "metadata": {
        "id": "5Dsngi2Fwuj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpbtKhU0QrZQ"
      },
      "outputs": [],
      "source": [
        "data_obj_list = from_json_to_heteroData_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ooUvHpMwD_C",
        "outputId": "c65852d5-47ee-4785-83f4-772b4a212979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3029 sequences available from 106 valid scenes. \n",
            "115 non valid scens for a sequence length of 20\n"
          ]
        }
      ],
      "source": [
        "non_valid, valid, tot = calc_possible_seq(seq_length, data_obj_list)\n",
        "print(f\"{tot} sequences available from {valid} valid scenes. \\n{non_valid} non valid scens for a sequence length of {seq_length}\")\n",
        "dataset = HeteroDataset(create_samples(data_obj_list, seq_length))\n",
        "metadata = dataset[0].metadata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzk4LUR8recZ",
        "outputId": "3a0e9926-0120-421e-ec66-f9815c683e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The seed is: 3260\n",
            "Training set size: 2100\n",
            "Validation set size: 450\n",
            "Testing set size: 450\n"
          ]
        }
      ],
      "source": [
        "seed = random.randint(0, 10000)\n",
        "print(f\"The seed is: {seed}\")\n",
        "random.seed(seed)\n",
        "\n",
        "subset = random.sample(dataset.data_list, samples)\n",
        "\n",
        "random.shuffle(subset)\n",
        "\n",
        "num_train = int(0.70 * len(subset))  # 70% for training\n",
        "num_val = int(0.15 * len(subset))    # 15% for validation\n",
        "num_test = len(subset) - num_train - num_val  # 15% for testing\n",
        "\n",
        "train_data = subset[:num_train]\n",
        "val_data = subset[num_train:num_train + num_val]\n",
        "test_data = subset[num_train + num_val:]\n",
        "\n",
        "train_dataset = HeteroDataset(train_data)\n",
        "val_dataset = HeteroDataset(val_data)\n",
        "test_dataset = HeteroDataset(test_data)\n",
        "\n",
        "# Print sizes to verify\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Testing set size: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzuAeIpf-Dv3"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNJsEdw425-2"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch = batch.to(device)\n",
        "model = TransformerWithClassifier(metadata, hidden_channels, out_channels).to(device)\n",
        "a, b = model(batch.x_dict, batch.edge_index_dict)\n",
        "if init_weights:\n",
        "  init_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvbp_VGU7Vr-"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "total_epochs = epochs\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    if epoch + 1 < epoch_max:\n",
        "        return epoch / epoch_max\n",
        "    else:\n",
        "        return (total_epochs - epoch) / (total_epochs - epoch_max)\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I4RUTvB1RN5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "        settings=wandb.Settings(start_method=\"fork\"),\n",
        "        project=\"EASG GNN\",\n",
        "        config={\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"lr\": lr,\n",
        "            \"model_name\": type(model).__name__,\n",
        "            \"sequence_length\": seq_length,\n",
        "            \"extra_features\": extra_features,\n",
        "            \"trained_loss\": trained_loss,\n",
        "            \"init_weights\": init_weights,\n",
        "            \"loss_weights\": loss_weights,\n",
        "            \"word2vec\": word2vec,\n",
        "            \"other objects\": include_future_object,\n",
        "            \"extracted features\": include_extracted_features\n",
        "            })\n",
        "\n",
        "config = wandb.config\n",
        "history = train(model, train_loader, optimizer, scheduler, device, epochs, val_loader=val_loader, extra_features=extra_features, loss_weights=loss_weights)\n",
        "test(model, test_loader, device, extra_features=extra_features, loss_weights=loss_weights)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Wl8KUAHXPv6C",
        "ukL61VIJQEqN",
        "m4FE-ptpP9nN",
        "YLuwzrFMQgSQ",
        "BlLlU4paRGl6",
        "e0Q_ienGCrDr",
        "x8szjkN8LBN5",
        "WwA14VH1CzOY",
        "87NNXoZqC3RK",
        "48c77gbx0_D6",
        "itUQ2skn8WtN",
        "9gd2dOqI8mcZ",
        "4aKd1XBt84g0",
        "72s8UxKj9FAR",
        "Nh1fafIO-iMK"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}